"""
Advanced Vulnerability Analysis Module for TWX

This module provides comprehensive analysis capabilities for vulnerability data.
It works independently from the classifier module and provides multiple analysis types.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Add the project root directory to Python path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(root_dir)

from storage.vulnerability_db import VulnerabilityDatabase


class VulnerabilityAnalyzer:
    """Advanced vulnerability data analyzer with multiple analysis capabilities."""
    
    def __init__(self, data_path="analysis/classified_vulnerabilities_with_details.csv"):
        """
        Initialize the analyzer with vulnerability data.
        
        Args:
            data_path: Path to the classified vulnerability data CSV
        """
        self.df = pd.read_csv(data_path, low_memory=False, parse_dates=['published_date', 'modified_date'])
        print(f"Loaded {len(self.df)} vulnerability records for analysis")
        
        # Filter out Unknown and Other by default
        self.exclude_types = ["Unknown", "Other"]
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)]
        
        # Define output directory
        self.output_dir = "analysis/reports"
        os.makedirs(self.output_dir, exist_ok=True)
    
    def analyze_vulnerability_landscape(self):
        """Analyze the vulnerability landscape including prevalence and severity."""
        print("Analyzing vulnerability landscape...")
        
        # Define severity weights mapping
        severity_weights = {
            "Low": 1,
            "Medium": 3,
            "High": 6,
            "Critical": 10
        }
        
        # Filter the dataframe if exclusions are specified
        if self.exclude_types:
            filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)]
            print(f"Excluded {len(self.df) - len(filtered_df)} records with types: {self.exclude_types}")
        else:
            filtered_df = self.df
        
        # Vulnerability type distribution (prevalence)
        plt.figure(figsize=(14, 8))
        type_counts = filtered_df["vuln_type"].value_counts()
        
        # Plot the bar chart
        sns.barplot(y=type_counts.index, x=type_counts.values, color="steelblue")
        plt.title("Distribution of Vulnerability Types (Prevalence)")
        plt.xlabel("Count")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/vuln_type_distribution.png")
        
        # Create a dataframe with type, count and weighted score
        risk_scores = []
        for vuln_type in filtered_df["vuln_type"].unique():
            type_df = filtered_df[filtered_df["vuln_type"] == vuln_type]
            count = len(type_df)
            
            # Handle missing values more robustly
            valid_severities = type_df["severity"].dropna()
            if len(valid_severities) > 0:
                # Convert to string type before mapping to avoid categorical issues
                severity_values = valid_severities.astype(str).map(severity_weights).fillna(0) 
                weighted_score = sum(severity_values) / len(valid_severities)
            else:
                weighted_score = 0
            
            risk_scores.append({
                "vuln_type": vuln_type,
                "count": count,
                "avg_severity_weight": weighted_score,
                "risk_score": count * weighted_score  # Combines prevalence and severity
            })
        
        risk_df = pd.DataFrame(risk_scores).sort_values("risk_score", ascending=False)
        
        # Plot combined risk score
        plt.figure(figsize=(14, 8))
        sns.barplot(y="vuln_type", x="risk_score", data=risk_df, color="purple")
        plt.title("Combined Risk Score by Vulnerability Type (Prevalence Ã— Severity)")
        plt.xlabel("Risk Score")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/vuln_type_risk_score.png")
        
        # Save results
        risk_df.to_csv(f"{self.output_dir}/vulnerability_risk_analysis.csv", index=False)
        print(f"Risk analysis saved to {self.output_dir}/vulnerability_risk_analysis.csv")
        
        print("\nTop 5 vulnerability types by combined risk score:")
        print(risk_df[["vuln_type", "count", "avg_severity_weight", "risk_score"]].head(5))
        
        return risk_df
    
    def analyze_temporal_trends(self):
        """Analyze how vulnerability types have evolved over time."""
        print("Analyzing temporal vulnerability trends...")
        
        # Extract year from published_date
        self.filtered_df['year'] = pd.to_datetime(self.filtered_df['published_date']).dt.year
        
        # Group by year and vulnerability type
        yearly_trends = self.filtered_df.groupby(['year', 'vuln_type']).size().unstack().fillna(0)
        
        # Plot the trends
        plt.figure(figsize=(15, 8))
        yearly_trends.plot(kind='line', marker='o')
        plt.title('Evolution of Vulnerability Types Over Time')
        plt.xlabel('Year')
        plt.ylabel('Count')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(title="Vulnerability Type", bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/temporal_vulnerability_trends.png")
        
        # Calculate year-over-year growth rates for each vulnerability type
        growth_rates = pd.DataFrame()
        for vuln_type in yearly_trends.columns:
            yearly_data = yearly_trends[vuln_type]
            growth = yearly_data.pct_change() * 100  # Convert to percentage
            growth_rates[vuln_type] = growth
        
        # Save to CSV
        yearly_trends.to_csv(f"{self.output_dir}/vulnerability_trends_by_year.csv")
        growth_rates.to_csv(f"{self.output_dir}/vulnerability_growth_rates.csv")
        
        return yearly_trends, growth_rates
    
    def analyze_complexity(self):
        """Analyze relationship between CVSS metrics and vulnerability types."""
        print("Analyzing vulnerability complexity...")
        
        # Create complexity score based on Attack Complexity and User Interaction
        # Lower score means easier to exploit
        complexity_map = {'L': 0, 'H': 1}  # Low complexity = 0, High complexity = 1
        ui_map = {'N': 0, 'R': 1}  # No UI = 0, Required UI = 1
        
        # Ensure columns exist and handle missing values
        if 'ac' in self.filtered_df.columns and 'ui' in self.filtered_df.columns:
            valid_df = self.filtered_df[self.filtered_df['ac'].isin(['L', 'H']) & 
                                       self.filtered_df['ui'].isin(['N', 'R'])]
            
            valid_df['complexity_score'] = valid_df['ac'].map(complexity_map) + valid_df['ui'].map(ui_map)
            
            # Group by vulnerability type and calculate average complexity
            complexity_by_type = valid_df.groupby('vuln_type')['complexity_score'].agg(
                ['mean', 'count', 'std']
            ).sort_values('mean')
            
            # Only include types with sufficient samples
            min_samples = 5
            complexity_by_type = complexity_by_type[complexity_by_type['count'] >= min_samples]
            
            # Plot the results
            plt.figure(figsize=(14, 8))
            ax = complexity_by_type['mean'].plot(kind='barh', xerr=complexity_by_type['std'], 
                                    capsize=5, color='steelblue')
            plt.title('Average Exploitation Complexity by Vulnerability Type')
            plt.ylabel('Vulnerability Type')
            plt.xlabel('Complexity Score (lower is simpler to exploit)')
            plt.grid(True, linestyle='--', axis='x', alpha=0.7)
            
            # Add count annotations
            for i, (idx, row) in enumerate(complexity_by_type.iterrows()):
                plt.text(0.01, i, f"n={int(row['count'])}", va='center')
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/vulnerability_complexity.png")
            
            # Save to CSV
            complexity_by_type.to_csv(f"{self.output_dir}/vulnerability_complexity.csv")
            
            return complexity_by_type
        else:
            print("Warning: 'ac' or 'ui' columns missing, skipping complexity analysis")
            return None
    
    def create_risk_matrix(self):
        """Create a risk prioritization matrix based on severity and exploitability."""
        print("Creating risk prioritization matrix...")
        
        # Check if needed columns exist
        required_cols = ['base_score', 'exploitability_score', 'has_exploit']
        if not all(col in self.filtered_df.columns for col in required_cols):
            print(f"Warning: One or more required columns missing: {required_cols}")
            print("Skipping risk matrix analysis")
            return None
        
        # Prepare data: severity, exploitability, whether exploit exists
        matrix_data = self.filtered_df[required_cols].copy()
        matrix_data = matrix_data.fillna(0)
        
        # Normalize data for clustering
        scaler = StandardScaler()
        matrix_data_scaled = scaler.fit_transform(matrix_data)
        
        # Apply K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        self.filtered_df['risk_cluster'] = kmeans.fit_predict(matrix_data_scaled)
        
        # Map clusters to risk levels
        cluster_centers = kmeans.cluster_centers_
        risk_importance = np.zeros(4)
        
        # Calculate risk importance for each cluster (higher is more risky)
        for i in range(4):
            # Base score is most important, then exploitability, then exploit existence
            risk_importance[i] = (cluster_centers[i, 0] * 0.5 +  # base_score (50% weight)
                                 cluster_centers[i, 1] * 0.3 +   # exploitability (30% weight)
                                 cluster_centers[i, 2] * 0.2)    # has_exploit (20% weight)
        
        # Sort clusters by risk importance (highest first)
        risk_mapping = {i: rank for rank, i in enumerate(np.argsort(risk_importance)[::-1])}
        risk_labels = ['Critical', 'High', 'Medium', 'Low']
        
        self.filtered_df['risk_priority'] = self.filtered_df['risk_cluster'].map(
            {i: risk_labels[risk_mapping[i]] for i in range(4)}
        )
        
        # Plot the matrix
        plt.figure(figsize=(12, 10))
        
        # Convert has_exploit to numeric if needed
        if self.filtered_df['has_exploit'].dtype == bool:
            exploit_size = self.filtered_df['has_exploit'].astype(int) * 50 + 50
        else:
            exploit_size = self.filtered_df['has_exploit'].astype(float) * 50 + 50
        
        scatter = plt.scatter(
            self.filtered_df['base_score'], 
            self.filtered_df['exploitability_score'], 
            c=self.filtered_df['risk_cluster'], 
            cmap='viridis',
            s=exploit_size, 
            alpha=0.7
        )
        
        # Add colorbar legend
        cbar = plt.colorbar(scatter, label='Risk Cluster')
        cbar.set_ticks(range(4))
        cbar.set_ticklabels([f"{risk_labels[risk_mapping[i]]}" for i in range(4)])
        
        # Add cluster centers
        plt.scatter(
            kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            c='red', 
            marker='X', 
            s=200, 
            label='Cluster Centers'
        )
        
        plt.title('Risk Prioritization Matrix')
        plt.xlabel('CVSS Base Score')
        plt.ylabel('Exploitability Score')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/risk_prioritization_matrix.png")
        
        # Save risk distribution
        risk_dist = self.filtered_df['risk_priority'].value_counts().sort_index()
        risk_by_type = self.filtered_df.groupby(['vuln_type', 'risk_priority']).size().unstack().fillna(0)
        risk_by_type_pct = risk_by_type.div(risk_by_type.sum(axis=1), axis=0) * 100
        
        # Save to CSV
        risk_priority_df = self.filtered_df[['vuln_id', 'vuln_type', 'risk_priority', 
                                             'base_score', 'exploitability_score', 'has_exploit']]
        risk_priority_df.to_csv(f"{self.output_dir}/vulnerability_risk_priorities.csv", index=False)
        risk_by_type.to_csv(f"{self.output_dir}/risk_distribution_by_type.csv")
        risk_by_type_pct.to_csv(f"{self.output_dir}/risk_distribution_by_type_percent.csv")
        
        return risk_priority_df, risk_by_type
    
    def analyze_attack_chains(self):
        """Analyze potential attack chains across different systems."""
        print("Analyzing attack chains...")
        
        # Define attack stages
        attack_stages = {
            'Initial Access': ['Information Disclosure', 'Authentication Issues', 
                               'Open Redirect', 'Input Validation'],
            'Execution': ['Command Injection', 'Code Injection', 'Cross-site Scripting (XSS)', 
                         'SQL Injection', 'Deserialization of Untrusted Data'],
            'Privilege Escalation': ['Access Control Issues', 'Privilege Escalation', 
                                    'Path Traversal', 'SSRF'],
            'Data Exfiltration': ['Information Disclosure', 'Cryptographic Issues', 
                                 'Cleartext Transmission']
        }
        
        # Map vulnerabilities to attack stages
        for stage, vuln_types in attack_stages.items():
            stage_col = f'is_{stage.lower().replace(" ", "_")}'
            self.filtered_df[stage_col] = self.filtered_df['vuln_type'].isin(vuln_types)
        
        # Ensure we have product and vendor columns
        if 'product' not in self.filtered_df.columns or 'vendor' not in self.filtered_df.columns:
            print("Warning: product or vendor columns missing, skipping attack chain analysis")
            return None
        
        # Group by product to see which have vulnerabilities in multiple stages
        product_stages = self.filtered_df.groupby(['vendor', 'product'])[
            [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        ].sum()
        
        # Products with complete attack chains have vulnerabilities in all stages
        stage_cols = [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        product_stages['attack_chain_completeness'] = (
            product_stages[stage_cols].apply(lambda x: sum(x > 0) / len(attack_stages), axis=1)
        )
        
        # Add vulnerability counts
        product_vuln_count = self.filtered_df.groupby(['vendor', 'product']).size()
        product_stages['total_vulnerabilities'] = product_vuln_count
        
        # Sort by attack chain completeness and then by total vulnerabilities 
        product_stages = product_stages.sort_values(
            ['attack_chain_completeness', 'total_vulnerabilities'], 
            ascending=[False, False]
        )
        
        # Create a visualization of the top N products with most complete attack chains
        top_products = product_stages.head(20)
        
        # Plot attack chain completeness for top products
        plt.figure(figsize=(15, 10))
        
        # Create a more readable index for plotting
        plot_index = [f"{v} {p}" for v, p in top_products.index]
        
        # Plot the completeness as a horizontal bar chart
        ax = plt.barh(plot_index, top_products['attack_chain_completeness'], color='steelblue')
        
        # Add total vulnerability count as text
        for i, (idx, row) in enumerate(top_products.iterrows()):
            plt.text(
                row['attack_chain_completeness'] + 0.01, 
                i, 
                f"({int(row['total_vulnerabilities'])} vulns)", 
                va='center'
            )
        
        plt.title('Top Products by Attack Chain Completeness')
        plt.xlabel('Attack Chain Completeness (proportion of attack stages covered)')
        plt.ylabel('Product')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        plt.xlim(0, 1.1)  # Make room for annotations
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/attack_chain_completeness.png")
        
        # Save detailed product stage data
        product_stages.to_csv(f"{self.output_dir}/product_attack_stages.csv")
        
        # Also create a heatmap for the top products showing which stages they're vulnerable to
        plt.figure(figsize=(16, 12))
        
        # Prepare data for heatmap (boolean to show which stages each product has vulns for)
        heatmap_data = top_products[stage_cols].applymap(lambda x: 1 if x > 0 else 0)
        heatmap_data.columns = [col.replace('is_', '').replace('_', ' ').title() for col in heatmap_data.columns]
        
        sns.heatmap(heatmap_data, cmap=['white', 'darkred'], cbar=False, 
                   linewidths=1, linecolor='lightgray', 
                   yticklabels=plot_index)
        plt.title('Attack Stages Coverage by Top Products')
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/attack_stages_heatmap.png")
        
        return product_stages
    
    def analyze_mitigation_coverage(self):
        """Analyze which vulnerability types have the most comprehensive mitigations."""
        print("Analyzing mitigation coverage...")
        
        # Check if mitigation_info column exists
        if 'mitigation_info' not in self.filtered_df.columns:
            print("Warning: 'mitigation_info' column missing, skipping mitigation coverage analysis")
            return None
        
        # Calculate mitigation coverage (how complete/detailed the mitigation info is)
        # This assumes mitigation_info has been parsed into a usable format
        self.filtered_df['has_mitigation'] = self.filtered_df['mitigation_info'].apply(
            lambda x: 0 if pd.isna(x) or x == {} or x == '' else 1
        )
        
        # Group by vulnerability type and calculate coverage percentage
        mitigation_coverage = self.filtered_df.groupby('vuln_type').agg(
            coverage_pct=('has_mitigation', lambda x: sum(x) / len(x) * 100),
            total_vulns=('vuln_type', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        plt.figure(figsize=(14, 8))
        ax = mitigation_coverage['coverage_pct'].plot(kind='barh', color='steelblue')
        plt.title('Mitigation Coverage by Vulnerability Type')
        plt.xlabel('Coverage Percentage')
        plt.ylabel('Vulnerability Type')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(mitigation_coverage.iterrows()):
            plt.text(1, i, f"n={row['total_vulns']}", va='center')
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/mitigation_coverage.png")
        
        # Save to CSV
        mitigation_coverage.to_csv(f"{self.output_dir}/mitigation_coverage.csv")
        
        return mitigation_coverage
    
    def analyze_zero_day_windows(self):
        """Analyze the window between vulnerability publication and patch availability."""
        print("Analyzing zero-day windows...")
        
        # Check if days_to_patch column exists
        if 'days_to_patch' not in self.filtered_df.columns:
            print("Warning: 'days_to_patch' column missing, skipping zero-day window analysis")
            return None
        
        # Create a true/false "has patch" column based on days_to_patch
        self.filtered_df['has_patch'] = ~self.filtered_df['days_to_patch'].isna() & (self.filtered_df['days_to_patch'] >= 0)
        
        # For vulnerabilities with patches, analyze the waiting period
        patched_vulns = self.filtered_df[self.filtered_df['has_patch']]
        
        if len(patched_vulns) == 0:
            print("No patched vulnerabilities found, skipping zero-day window analysis")
            return None
        
        # Group by vulnerability type and calculate statistics
        patch_stats = patched_vulns.groupby('vuln_type')['days_to_patch'].agg(
            ['mean', 'median', 'min', 'max', 'count']
        ).sort_values('mean')
        
        # Calculate the percentage of vulnerabilities that have patches
        patch_coverage = self.filtered_df.groupby('vuln_type')['has_patch'].agg(
            coverage_pct=('has_patch', lambda x: sum(x) / len(x) * 100),
            total_vulns=('has_patch', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))
        
        # Plot average days to patch
        patch_stats['mean'].plot(kind='barh', ax=ax1, color='steelblue')
        ax1.set_title('Average Days to Patch by Vulnerability Type')
        ax1.set_xlabel('Days')
        ax1.set_ylabel('Vulnerability Type')
        ax1.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_stats.iterrows()):
            ax1.text(1, i, f"n={int(row['count'])}", va='center')
        
        # Plot patch coverage
        patch_coverage['coverage_pct'].plot(kind='barh', ax=ax2, color='darkgreen')
        ax2.set_title('Patch Coverage by Vulnerability Type (%)')
        ax2.set_xlabel('Coverage Percentage')
        ax2.set_ylabel('Vulnerability Type')
        ax2.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_coverage.iterrows()):
            ax2.text(1, i, f"n={int(row['total_vulns'])}", va='center')
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/patch_analysis.png")
        
        # Save to CSV
        patch_stats.to_csv(f"{self.output_dir}/vulnerability_patch_timing.csv")
        patch_coverage.to_csv(f"{self.output_dir}/vulnerability_patch_coverage.csv")
        
        return patch_stats, patch_coverage
    
    def run_all_analyses(self):
        """Run all analyses and return results."""
        results = {}
        
        # Run basic analysis first
        print("\n1. Running basic vulnerability landscape analysis...")
        results['basic'] = self.analyze_vulnerability_landscape()
        
        # Run the six advanced analyses
        print("\n2. Analyzing temporal vulnerability trends...")
        results['temporal'] = self.analyze_temporal_trends()
        
        print("\n3. Analyzing vulnerability complexity...")
        results['complexity'] = self.analyze_complexity()
        
        print("\n4. Creating risk prioritization matrix...")
        results['risk_matrix'] = self.create_risk_matrix()
        
        print("\n5. Analyzing attack chains...")
        results['attack_chains'] = self.analyze_attack_chains()
        
        print("\n6. Analyzing mitigation coverage...")
        results['mitigation'] = self.analyze_mitigation_coverage()
        
        print("\n7. Analyzing zero-day windows...")
        results['zero_day'] = self.analyze_zero_day_windows()
        
        print(f"\nAll analyses complete. Reports saved to {self.output_dir}/")
        return results
    
    def set_exclude_types(self, exclude_types=None):
        """
        Update which vulnerability types to exclude from analysis.
        
        Args:
            exclude_types: List of vulnerability types to exclude or None to reset to default
        """
        if exclude_types is None:
            self.exclude_types = ["Unknown", "Other"]
        else:
            self.exclude_types = exclude_types
        
        # Update filtered dataframe
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)]
        print(f"Updated exclusion list. Now excluding: {self.exclude_types}")
        print(f"Filtered dataset contains {len(self.filtered_df)} records")


if __name__ == "__main__":
    # Parse command line arguments if any
    import argparse
    
    parser = argparse.ArgumentParser(description='TWX Advanced Vulnerability Analysis')
    parser.add_argument('--data', type=str, default="analysis/classified_vulnerabilities_with_details.csv",
                      help='Path to the classified vulnerability data CSV')
    parser.add_argument('--analyses', type=str, default="all",
                      help='Comma-separated list of analyses to run, or "all"')
    parser.add_argument('--exclude', type=str, default=None,
                      help='Comma-separated list of vulnerability types to exclude')
    parser.add_argument('--output', type=str, default="analysis/reports",
                      help='Directory where reports will be saved')
    
    args = parser.parse_args()
    
    # Initialize the analyzer
    analyzer = VulnerabilityAnalyzer(data_path=args.data)
    
    # Set output directory if specified
    if args.output:
        analyzer.output_dir = args.output
        os.makedirs(analyzer.output_dir, exist_ok=True)
    
    # Set exclusion types if specified
    if args.exclude:
        exclude_list = [t.strip() for t in args.exclude.split(',')]
        analyzer.set_exclude_types(exclude_list)
    
    # Run analyses based on arguments
    if args.analyses.lower() == "all":
        analyzer.run_all_analyses()
    else:
        # Run selected analyses
        analyses = args.analyses.split(',')
        for analysis in analyses:
            analysis = analysis.strip().lower()
            if analysis == "basic":
                analyzer.analyze_vulnerability_landscape()
            elif analysis == "temporal":
                analyzer.analyze_temporal_trends()
            elif analysis == "complexity":
                analyzer.analyze_complexity()
            elif analysis == "risk":
                analyzer.create_risk_matrix()
            elif analysis == "attack_chains":
                analyzer.analyze_attack_chains()
            elif analysis == "mitigation":
                analyzer.analyze_mitigation_coverage()
            elif analysis == "zero_day":
                analyzer.analyze_zero_day_windows()
            else:
                print(f"Warning: Unknown analysis type '{analysis}'")