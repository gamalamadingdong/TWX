"""
Advanced Vulnerability Analysis Module for TWX

This module provides comprehensive analysis capabilities for vulnerability data.
It works independently from the classifier module and provides multiple analysis types.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Add the project root directory to Python path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(root_dir)

from storage.vulnerability_db import VulnerabilityDatabase


class VulnerabilityAnalyzer:
    """Advanced vulnerability data analyzer with multiple analysis capabilities."""
    
    def __init__(self, data_path="analysis/classification_data.csv", 
                 vendor_filter=None, product_filter=None):
        """Initialize the analyzer with vulnerability data from JSON."""
        # Check if file exists and is JSON
        if os.path.exists(data_path) and data_path.lower().endswith('.json'):
            # Load from JSON
            self.df = pd.read_json(data_path, orient='records')
            print(f"Loaded {len(self.df)} vulnerability records from JSON")
        else:
            # Fall back to CSV if JSON not found or for backward compatibility
            csv_path = data_path.replace('.json', '.csv') if data_path.lower().endswith('.json') else data_path
            if os.path.exists(csv_path):
                self.df = pd.read_csv(csv_path, low_memory=False, parse_dates=['published', 'modified'])
                print(f"Loaded {len(self.df)} vulnerability records from CSV (fallback)")
            else:
                raise FileNotFoundError(f"Could not find data at {data_path} or {csv_path}")
        
        # Convert date columns
        for date_col in ['published', 'modified', 'patch_date']:
            if date_col in self.df.columns:
                self.df[date_col] = pd.to_datetime(self.df[date_col], errors='coerce')
        # Create vuln_type if it doesn't exist in the data
        if "vuln_type" not in self.df.columns:
            print("'vuln_type' column not found - creating from CWE data")
            
            if "primary_cwe" in self.df.columns:
                # Map CWE to vulnerability types using our categorization
                cwe_type_map = {
                    # Memory Safety
                    "CWE-119": "Buffer Overflow",
                    "CWE-120": "Buffer Overflow", 
                    "CWE-125": "Buffer Overflow",
                    "CWE-787": "Buffer Overflow",
                    "CWE-416": "Use After Free",
                    "CWE-415": "Double Free",
                    "CWE-476": "Null Pointer Dereference",
                    
                    # Injection
                    "CWE-89": "SQL Injection",
                    "CWE-564": "SQL Injection",
                    "CWE-77": "Command Injection",
                    "CWE-78": "Command Injection",
                    "CWE-79": "Cross-site Scripting",
                    "CWE-80": "Cross-site Scripting",
                    "CWE-83": "Cross-site Scripting",
                    "CWE-91": "XML Injection",
                    "CWE-643": "XML Injection",
                    "CWE-90": "LDAP Injection",
                    "CWE-134": "Format String",
                    
                    # Access Control
                    "CWE-284": "Access Control",
                    "CWE-285": "Access Control",
                    "CWE-287": "Authentication Issues",
                    "CWE-306": "Authentication Issues",
                    "CWE-22": "Path Traversal",
                    "CWE-23": "Path Traversal",
                    "CWE-36": "Path Traversal",
                    "CWE-269": "Privilege Escalation",
                    "CWE-732": "Incorrect Permissions",
                    
                    # Other common types
                    "CWE-200": "Information Disclosure",
                    "CWE-20": "Input Validation", 
                    "CWE-352": "Cross-Site Request Forgery",
                    "CWE-434": "Unrestricted File Upload",
                    "CWE-94": "Code Injection",
                    "CWE-400": "Resource Management"
                }
                
                # Apply categorization with fallback for any CWE not in our map
                def map_cwe_to_type(cwe):
                    if pd.isna(cwe) or not cwe:
                        return "Unknown"
                    
                    if cwe in cwe_type_map:
                        return cwe_type_map[cwe]
                        
                    # Fallbacks based on CWE number ranges 
                    if cwe.startswith("CWE-"):
                        try:
                            cwe_num = int(cwe[4:])
                            
                            # Use number ranges for common categories
                            if 119 <= cwe_num <= 127:  # Memory buffer errors
                                return "Buffer Overflow"
                            elif 74 <= cwe_num <= 94:   # Injection issues
                                return "Injection"
                            elif 264 <= cwe_num <= 286:  # Access/permissions issues
                                return "Access Control"
                            elif 310 <= cwe_num <= 340:  # Crypto issues
                                return "Cryptographic Issues"
                            elif 200 <= cwe_num <= 213:  # Information disclosure
                                return "Information Disclosure"
                        except ValueError:
                            pass
                    
                    return "Other"
                    
                self.df["vuln_type"] = self.df["primary_cwe"].apply(map_cwe_to_type)
                
            elif "cwe_ids" in self.df.columns:
                # If we have a list of CWEs but no primary_cwe, extract the first one
                def extract_first_cwe(cwe_str):
                    if pd.isna(cwe_str) or not cwe_str:
                        return "Unknown"
                        
                    # Parse the string representation of CWE list
                    if isinstance(cwe_str, str):
                        # Handle formats like "['CWE-79', 'CWE-89']" or "CWE-79,CWE-89"
                        if ',' in cwe_str:
                            # Extract first CWE from comma-separated list
                            cwe_list = [cwe.strip().strip("'[]\"") for cwe in cwe_str.split(',')]
                            if cwe_list:
                                first_cwe = cwe_list[0]
                                if 'CWE-' in first_cwe:
                                    return first_cwe
                        elif 'CWE-' in cwe_str:
                            # Direct CWE ID 
                            return cwe_str.strip().strip("'[]\"")
                    
                    return "Unknown"
                    
                # Extract first CWE and then map to type
                self.df["primary_cwe"] = self.df["cwe_ids"].apply(extract_first_cwe)
                self.df["vuln_type"] = self.df["primary_cwe"].apply(map_cwe_to_type)
            
            else:
                # If we have no CWE data, set Unknown
                self.df["vuln_type"] = "Unknown"
        
        # Filter out Unknown and Other by default
        self.exclude_types = ["Unknown", "Other"]
        
        # Create a proper copy to avoid SettingWithCopyWarning
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        
        # Initialize filter description (for output filenames)
        self.filter_description = None
        
        # Apply vendor/product filters if specified
        if vendor_filter or product_filter:
            self.filter_by_vendor_product(vendor_filter, product_filter)
        
        # Define output directory
        self.output_dir = "analysis/reports"
        os.makedirs(self.output_dir, exist_ok=True)

    def filter_by_vendor_product(self, vendor_filter=None, product_filter=None):
        """
        Filter the dataset by vendor and/or product using smart text matching.
        
        Args:
            vendor_filter: String or list of strings to filter vendors (case-insensitive partial match)
            product_filter: String or list of strings to filter products (case-insensitive partial match)
        
        Returns:
            Number of records matching the filter criteria
        """
        # Start with the full dataset filtered by vulnerability type
        filtered_data = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        
        # Check if we have vendor and product columns
        if 'vendors' not in filtered_data.columns or 'products' not in filtered_data.columns:
            print("Warning: 'vendor' or 'product' columns missing, cannot apply filters")
            self.filtered_df = filtered_data
            return len(filtered_data)
        
        # Initial counts
        initial_count = len(filtered_data)
        
        # Apply vendor filter if specified
        if vendor_filter:
            vendor_matches = self._smart_text_match(filtered_data['vendors'], vendor_filter)
            filtered_data = filtered_data[vendor_matches].copy()
            print(f"Applied vendor filter: {len(filtered_data)} of {initial_count} records match")
        
        # Apply product filter if specified
        if product_filter:
            product_matches = self._smart_text_match(filtered_data['products'], product_filter)
            filtered_data = filtered_data[product_matches].copy()
            print(f"Applied product filter: {len(filtered_data)} of {initial_count} records match")
        
        # Update the filtered dataframe
        self.filtered_df = filtered_data
        
        # Generate filter description for reports
        filter_desc = []
        if vendor_filter:
            if isinstance(vendor_filter, list):
                filter_desc.append(f"vendors={','.join(vendor_filter)}")
            else:
                filter_desc.append(f"vendor={vendor_filter}")
        
        if product_filter:
            if isinstance(product_filter, list):
                filter_desc.append(f"products={','.join(product_filter)}")
            else:
                filter_desc.append(f"product={product_filter}")
        
        # Store filter description for file naming
        self.filter_description = "_".join(filter_desc) if filter_desc else None
        
        return len(filtered_data)

    def analyze_complexity(self):
        """Analyze relationship between CVSS metrics and vulnerability types."""
        print("Analyzing vulnerability complexity...")
        
        # Create complexity score based on Attack Complexity and User Interaction
        # Lower score means easier to exploit
        complexity_map = {'L': 0, 'H': 1}  # Low complexity = 0, High complexity = 1
        ui_map = {'N': 0, 'R': 1}  # No UI = 0, Required UI = 1
        
        # Ensure columns exist and handle missing values
        if 'ac' in self.filtered_df.columns and 'ui' in self.filtered_df.columns:
            valid_df = self.filtered_df[self.filtered_df['ac'].isin(['L', 'H']) & 
                                       self.filtered_df['ui'].isin(['N', 'R'])].copy()
            
            valid_df.loc[:, 'complexity_score'] = valid_df['ac'].map(complexity_map) + valid_df['ui'].map(ui_map)
            
            # Group by vulnerability type and calculate average complexity
            complexity_by_type = valid_df.groupby('vuln_type')['complexity_score'].agg(
                ['mean', 'count', 'std']
            ).sort_values('mean')
            
            # Only include types with sufficient samples
            min_samples = 5
            complexity_by_type = complexity_by_type[complexity_by_type['count'] >= min_samples]
            
            # Plot the results
            plt.figure(figsize=(14, 8))
            ax = complexity_by_type['mean'].plot(kind='barh', xerr=complexity_by_type['std'], 
                                    capsize=5, color='steelblue')
            plt.title('Average Exploitation Complexity by Vulnerability Type')
            plt.ylabel('Vulnerability Type')
            plt.xlabel('Complexity Score (lower is simpler to exploit)')
            plt.grid(True, linestyle='--', axis='x', alpha=0.7)
            
            # Add count annotations
            for i, (idx, row) in enumerate(complexity_by_type.iterrows()):
                plt.text(0.01, i, f"n={int(row['count'])}", va='center')
            
            plt.tight_layout()
            
            # Use filter information in filename
            complexity_filename = self.get_output_filename("vulnerability_complexity")
            plt.savefig(f"{self.output_dir}/{complexity_filename}.png")
            
            # Save to CSV with filter information
            csv_filename = self.get_output_filename("vulnerability_complexity")
            complexity_by_type.to_csv(f"{self.output_dir}/{csv_filename}.csv")
            
            return complexity_by_type
        else:
            print("Warning: 'ac' or 'ui' columns missing, skipping complexity analysis")
            return None
    
    def create_risk_matrix(self):
        """Create a risk prioritization matrix based on severity and exploitability."""
        print("Creating risk prioritization matrix...")
        
        # Check if needed columns exist
        required_cols = ['base_score', 'exploitability_score', 'has_exploit']
        if not all(col in self.filtered_df.columns for col in required_cols):
            print(f"Warning: One or more required columns missing: {required_cols}")
            print("Skipping risk matrix analysis")
            return None
        
        # Prepare data: severity, exploitability, whether exploit exists
        matrix_data = self.filtered_df[required_cols].copy()
        matrix_data = matrix_data.fillna(0)
        
        # Normalize data for clustering
        scaler = StandardScaler()
        matrix_data_scaled = scaler.fit_transform(matrix_data)
        
        # Apply K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        self.filtered_df.loc[:, 'risk_cluster'] = kmeans.fit_predict(matrix_data_scaled)
        
        # Assign cluster labels to each row
        self.filtered_df['risk_cluster'] = kmeans.labels_

        # Find the most common vuln_type in each cluster
        cluster_type_map = {}
        for i in range(4):
            types = self.filtered_df[self.filtered_df['risk_cluster'] == i]['vuln_type']
            if not types.empty:
                top_types = types.value_counts().head(2).index.tolist()
                cluster_type_map[i] = ", ".join(top_types)
            else:
                cluster_type_map[i] = "N/A"
        
        # Map clusters to risk levels
        cluster_centers = kmeans.cluster_centers_
        risk_importance = np.zeros(4)
        
        # Calculate risk importance for each cluster (higher is more risky)
        for i in range(4):
            # Base score is most important, then exploitability, then exploit existence
            risk_importance[i] = (cluster_centers[i, 0] * 0.5 +  # base_score (50% weight)
                                 cluster_centers[i, 1] * 0.3 +   # exploitability (30% weight)
                                 cluster_centers[i, 2] * 0.2)    # has_exploit (20% weight)
        
        # Sort clusters by risk importance (highest first)
        risk_mapping = {i: rank for rank, i in enumerate(np.argsort(risk_importance)[::-1])}
        risk_labels = ['Critical', 'High', 'Medium', 'Low']
        
        self.filtered_df.loc[:, 'risk_priority'] = self.filtered_df['risk_cluster'].map(
            {i: risk_labels[risk_mapping[i]] for i in range(4)}
        )
        
        # Plot the matrix
        plt.figure(figsize=(12, 10))
        
        # Convert has_exploit to numeric if needed
        if self.filtered_df['has_exploit'].dtype == bool:
            exploit_size = self.filtered_df['has_exploit'].astype(int) * 50 + 50
        else:
            exploit_size = self.filtered_df['has_exploit'].astype(float) * 50 + 50
        
        scatter = plt.scatter(
            self.filtered_df['base_score'], 
            self.filtered_df['exploitability_score'], 
            c=self.filtered_df['risk_cluster'], 
            cmap='viridis',
            s=exploit_size, 
            alpha=0.7
        )
        
        # Add colorbar legend
        cbar = plt.colorbar(scatter, label='Risk Cluster')
        cbar.set_ticks(range(4))
        cbar.set_ticklabels([f"{risk_labels[risk_mapping[i]]}" for i in range(4)])
        
        # Add cluster centers
        for i, (x, y) in enumerate(zip(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1])):
            plt.scatter(x, y, c='red', marker='X', s=200, label='Cluster Centers' if i == 0 else "")
            plt.text(x, y, f"{risk_labels[risk_mapping[i]]}\n({cluster_type_map[i]})",
                     fontsize=10, fontweight='bold', color='black', ha='center', va='bottom', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))
        
        plt.title('Risk Prioritization Matrix')
        plt.xlabel('CVSS Base Score')
        plt.ylabel('Exploitability Score')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        
        # Use filter information in filename
        matrix_filename = self.get_output_filename("risk_prioritization_matrix")
        plt.savefig(f"{self.output_dir}/{matrix_filename}.png")
        
        # Save risk distribution
        risk_dist = self.filtered_df['risk_priority'].value_counts().sort_index()
        risk_by_type = self.filtered_df.groupby(['vuln_type', 'risk_priority']).size().unstack().fillna(0)
        risk_by_type_pct = risk_by_type.div(risk_by_type.sum(axis=1), axis=0) * 100
        
        # Save to CSV with filter information in filenames
        priorities_filename = self.get_output_filename("vulnerability_risk_priorities")
        dist_filename = self.get_output_filename("risk_distribution_by_type") 
        pct_filename = self.get_output_filename("risk_distribution_by_type_percent")
        
        risk_priority_df = self.filtered_df[['vuln_id', 'vuln_type', 'risk_priority', 
                                            'base_score', 'exploitability_score', 'has_exploit']]
        risk_priority_df.to_csv(f"{self.output_dir}/{priorities_filename}.csv", index=False)
        risk_by_type.to_csv(f"{self.output_dir}/{dist_filename}.csv")
        risk_by_type_pct.to_csv(f"{self.output_dir}/{pct_filename}.csv")
        
        return risk_priority_df, risk_by_type

    def analyze_attack_chains(self):
        """Analyze potential attack chains across different systems."""
        print("Analyzing attack chains...")
        
        # Define attack stages
        attack_stages = {
            'Initial Access': ['Information Disclosure', 'Authentication Issues', 
                              'Open Redirect', 'Input Validation'],
            'Execution': ['Command Injection', 'Code Injection', 'Cross-site Scripting (XSS)', 
                         'SQL Injection', 'Deserialization of Untrusted Data'],
            'Privilege Escalation': ['Access Control Issues', 'Privilege Escalation', 
                                    'Path Traversal', 'SSRF'],
            'Data Exfiltration': ['Information Disclosure', 'Cryptographic Issues', 
                                 'Cleartext Transmission']
        }
        
        # Map vulnerabilities to attack stages
        for stage, vuln_types in attack_stages.items():
            stage_col = f'is_{stage.lower().replace(" ", "_")}'
            self.filtered_df.loc[:, stage_col] = self.filtered_df['vuln_type'].isin(vuln_types)
        
        # Ensure we have product and vendor columns
        if 'products' not in self.filtered_df.columns or 'vendors' not in self.filtered_df.columns:
            print("Warning: product or vendor columns missing, skipping attack chain analysis")
            return None
        
        # Group by product to see which have vulnerabilities in multiple stages
        product_stages = self.filtered_df.groupby(['vendors', 'products'])[
            [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        ].sum()
        
        # Products with complete attack chains have vulnerabilities in all stages
        stage_cols = [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        product_stages['attack_chain_completeness'] = (
            product_stages[stage_cols].apply(lambda x: sum(x > 0) / len(attack_stages), axis=1)
        )
        
        # Add vulnerability counts
        product_vuln_count = self.filtered_df.groupby(['vendors', 'products']).size()
        product_stages['total_vulnerabilities'] = product_vuln_count
        
        # Sort by attack chain completeness and then by total vulnerabilities 
        product_stages = product_stages.sort_values(
            ['attack_chain_completeness', 'total_vulnerabilities'], 
            ascending=[False, False]
        )
        
        # Create a visualization of the top N products with most complete attack chains
        top_products = product_stages.head(20)
        
        # Plot attack chain completeness for top products
        plt.figure(figsize=(15, 10))
        
        # Create a more readable index for plotting
        plot_index = [f"{v} {p}" for v, p in top_products.index]
        
        # Plot the completeness as a horizontal bar chart
        ax = plt.barh(plot_index, top_products['attack_chain_completeness'], color='steelblue')
        
        # Add total vulnerability count as text
        for i, (idx, row) in enumerate(top_products.iterrows()):
            plt.text(
                row['attack_chain_completeness'] + 0.01, 
                i, 
                f"({int(row['total_vulnerabilities'])} vulns)", 
                va='center'
            )
        
        plt.title('Top Products by Attack Chain Completeness')
        plt.xlabel('Attack Chain Completeness (proportion of attack stages covered)')
        plt.ylabel('Product')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        plt.xlim(0, 1.1)  # Make room for annotations
        plt.tight_layout()
        
        # Use filter information in filename
        chain_filename = self.get_output_filename("attack_chain_completeness")
        plt.savefig(f"{self.output_dir}/{chain_filename}.png")
        
        # Save detailed product stage data with filter information
        stages_filename = self.get_output_filename("product_attack_stages")
        product_stages.to_csv(f"{self.output_dir}/{stages_filename}.csv")
        
        # Also create a heatmap for the top products showing which stages they're vulnerable to
        plt.figure(figsize=(16, 12))
        
        # Prepare data for heatmap (boolean to show which stages each product has vulns for)
        heatmap_data = top_products[stage_cols].copy()
        # Use apply column by column instead of deprecated applymap
        for col in heatmap_data.columns:
            heatmap_data[col] = heatmap_data[col].apply(lambda x: 1 if x > 0 else 0)
        heatmap_data.columns = [col.replace('is_', '').replace('_', ' ').title() for col in heatmap_data.columns]
        
        sns.heatmap(heatmap_data, cmap=['white', 'darkred'], cbar=False, 
                   linewidths=1, linecolor='lightgray', 
                   yticklabels=plot_index)
        plt.title('Attack Stages Coverage by Top Products')
        plt.tight_layout()
        
        # Use filter information in filename  
        heatmap_filename = self.get_output_filename("attack_stages_heatmap")
        plt.savefig(f"{self.output_dir}/{heatmap_filename}.png")
        
        # Compute average risk score (e.g., base_score) for each product and stage
        risk_matrix = pd.DataFrame(index=top_products.index, columns=stage_cols)
        for (vendor, product) in top_products.index:
            prod_mask = (self.filtered_df['vendors'] == vendor) & (self.filtered_df['products'] == product)
            for stage, col in zip(attack_stages, stage_cols):
                stage_mask = self.filtered_df['vuln_type'].isin(attack_stages[stage])
                relevant = self.filtered_df[prod_mask & stage_mask]
                if not relevant.empty and 'base_score' in relevant.columns:
                    risk_matrix.loc[(vendor, product), col] = relevant['base_score'].mean()
                else:
                    risk_matrix.loc[(vendor, product), col] = np.nan

        # Normalize risk scores for color mapping (optional, for better color scaling)
        risk_matrix = risk_matrix.astype(float)

        # Choose a color palette: green (low) → yellow → red (high)
        cmap = sns.color_palette("RdYlGn_r", as_cmap=True)

        # Prepare labels for the y-axis
        plot_index = [f"{v} {p}" for v, p in risk_matrix.index]
        risk_matrix.index = plot_index
        risk_matrix.columns = [col.replace('is_', '').replace('_', ' ').title() for col in risk_matrix.columns]

        plt.figure(figsize=(16, 12))
        sns.heatmap(
            risk_matrix,
            cmap=cmap,
            linewidths=1,
            linecolor='lightgray',
            yticklabels=plot_index,
            annot=True, fmt=".1f",  # Show average risk score in each cell
            cbar_kws={'label': 'Average Risk Score'}
        )
        plt.title('Attack Stages Coverage by Top Products (Colored by Average Risk Score)')
        plt.tight_layout()

        # Use filter information in filename  
        heatmap_filename = self.get_output_filename("attack_stages_risk_heatmap")
        plt.savefig(f"{self.output_dir}/{heatmap_filename}.png")
        
        return product_stages

    def analyze_mitigation_coverage(self):
        """Analyze which vulnerability types have the most comprehensive mitigations."""
        print("Analyzing mitigation coverage...")
        
        # Check if mitigation_info column exists
        if 'mitigation_info' not in self.filtered_df.columns:
            print("Warning: 'mitigation_info' column missing, skipping mitigation coverage analysis")
            return None
        
        # Calculate mitigation coverage (how complete/detailed the mitigation info is)
        # This assumes mitigation_info has been parsed into a usable format
        self.filtered_df.loc[:, 'has_mitigation'] = self.filtered_df['mitigation_info'].apply(
            lambda x: 0 if pd.isna(x) or x == {} or x == '' else 1
        )
        
        # Group by vulnerability type and calculate coverage percentage
        mitigation_coverage = self.filtered_df.groupby('vuln_type').agg(
            coverage_pct=('has_mitigation', lambda x: sum(x) / len(x) * 100),
            total_vulns=('vuln_type', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        plt.figure(figsize=(14, 8))
        ax = mitigation_coverage['coverage_pct'].plot(kind='barh', color='steelblue')
        plt.title('Mitigation Coverage by Vulnerability Type')
        plt.xlabel('Coverage Percentage')
        plt.ylabel('Vulnerability Type')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(mitigation_coverage.iterrows()):
            plt.text(1, i, f"n={row['total_vulns']}", va='center')
        
        plt.tight_layout()
        
        # Use filter information in filename
        coverage_filename = self.get_output_filename("mitigation_coverage")
        plt.savefig(f"{self.output_dir}/{coverage_filename}.png")
        
        # Save to CSV with filter information
        csv_filename = self.get_output_filename("mitigation_coverage")
        mitigation_coverage.to_csv(f"{self.output_dir}/{csv_filename}.csv")
        
        return mitigation_coverage

    def analyze_zero_day_windows(self):
        """Analyze the window between vulnerability publication and patch availability."""
        print("Analyzing zero-day windows...")
        
        # Check if days_to_patch column exists
        if 'days_to_patch' not in self.filtered_df.columns:
            print("Warning: 'days_to_patch' column missing, skipping zero-day window analysis")
            return None
        
        # Create a true/false "has patch" column based on days_to_patch
        self.filtered_df.loc[:, 'has_patch'] = ~self.filtered_df['days_to_patch'].isna() & (self.filtered_df['days_to_patch'] >= 0)
        
        # For vulnerabilities with patches, analyze the waiting period
        patched_vulns = self.filtered_df[self.filtered_df['has_patch']].copy()
        
        if len(patched_vulns) == 0:
            print("No patched vulnerabilities found, skipping zero-day window analysis")
            return None
        
        # Group by vulnerability type and calculate statistics
        patch_stats = patched_vulns.groupby('vuln_type')['days_to_patch'].agg(
            ['mean', 'median', 'min', 'max', 'count']
        ).sort_values('mean')
        
        # Calculate the percentage of vulnerabilities that have patches
        patch_coverage = self.filtered_df.groupby('vuln_type').agg(
            coverage_pct=('has_patch', lambda x: sum(x) / len(x) * 100),
            total_vulns=('has_patch', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))
        
        # Plot average days to patch
        patch_stats['mean'].plot(kind='barh', ax=ax1, color='steelblue')
        ax1.set_title('Average Days to Patch by Vulnerability Type')
        ax1.set_xlabel('Days')
        ax1.set_ylabel('Vulnerability Type')
        ax1.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_stats.iterrows()):
            ax1.text(1, i, f"n={int(row['count'])}", va='center')
        
        # Plot patch coverage
        patch_coverage['coverage_pct'].plot(kind='barh', ax=ax2, color='darkgreen')
        ax2.set_title('Patch Coverage by Vulnerability Type (%)')
        ax2.set_xlabel('Coverage Percentage')
        ax2.set_ylabel('Vulnerability Type')
        ax2.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_coverage.iterrows()):
            ax2.text(1, i, f"n={int(row['total_vulns'])}", va='center')
        
        plt.tight_layout()
        
        # Use filter information in filename
        patch_filename = self.get_output_filename("patch_analysis")
        plt.savefig(f"{self.output_dir}/{patch_filename}.png")
        
        # Save to CSV with filter information
        timing_filename = self.get_output_filename("vulnerability_patch_timing")
        coverage_filename = self.get_output_filename("vulnerability_patch_coverage")
        
        patch_stats.to_csv(f"{self.output_dir}/{timing_filename}.csv")
        patch_coverage.to_csv(f"{self.output_dir}/{coverage_filename}.csv")
        
        return patch_stats, patch_coverage

    def analyze_temporal_trends(self, months=None):
        """
        Analyze how vulnerability types have evolved over time.
        
        Args:
            months (int, optional): Number of months to look back. If None, all available data is used.
        """
        print("Analyzing temporal vulnerability trends...")
        
        # Create a working copy of the filtered dataframe
        df = self.filtered_df.copy()
        
        # Filter by time window if specified
        if months is not None and months > 0:
            # Find the cutoff date
            latest_date = df['published'].max()
            if pd.isna(latest_date):
                print("Warning: No valid dates found in dataset, using current date")
                latest_date = pd.Timestamp.now(tz='UTC')  # Add timezone info
            elif not isinstance(latest_date, pd.Timestamp):
                # Convert to timestamp if it's not already
                latest_date = pd.Timestamp(latest_date)
                
            # Check if latest_date is timezone-aware, if not make it UTC
            if latest_date.tzinfo is None:
                latest_date = latest_date.tz_localize('UTC')
                
            cutoff_date = latest_date - pd.DateOffset(months=months)
            
            # Make sure cutoff_date has the same timezone as published_date
            # Check a sample of published_date to see if it's timezone-aware
            sample_date = df['published'].dropna().iloc[0] if len(df['published'].dropna()) > 0 else None
            
            if sample_date is not None:
                if sample_date.tzinfo is None and cutoff_date.tzinfo is not None:
                    # If published_date is naive but cutoff is aware, make cutoff naive
                    cutoff_date = cutoff_date.tz_localize(None)
                elif sample_date.tzinfo is not None and cutoff_date.tzinfo is None:
                    # If published_date is aware but cutoff is naive, make cutoff aware
                    cutoff_date = cutoff_date.tz_localize(sample_date.tzinfo)
            
            # Filter data to only include records after the cutoff date
            df = df[df['published'] >= cutoff_date].copy()
            print(f"Filtered to vulnerabilities from the last {months} months ({len(df)} records)")
            
            if len(df) == 0:
                print("No data available for the specified time window")
                return None, None
        
        # Extract year and month for grouping
        df.loc[:, 'year_month'] = df['published'].dt.to_period('M')
        
        # Group by month and vulnerability type
        monthly_trends = df.groupby(['year_month', 'vuln_type']).size().unstack().fillna(0)
        
        # Ensure we have data
        if len(monthly_trends) == 0:
            print("No trend data available to plot")
            return None, None
        
        # Plot the trends
        plt.figure(figsize=(15, 8))
        ax = plt.gca()

        # Convert PeriodIndex to datetime for better plotting
        if isinstance(monthly_trends.index, pd.PeriodIndex):
            monthly_trends.index = monthly_trends.index.to_timestamp()

        # Use a color palette for better distinction
        palette = sns.color_palette("tab20", n_colors=len(monthly_trends.columns))
        color_list = palette.as_hex() if hasattr(palette, "as_hex") else palette

        # Only plot the top N types for clarity
        top_types = monthly_trends.sum().sort_values(ascending=False).head(8).index
        monthly_trends_to_plot = monthly_trends[top_types]

        monthly_trends_to_plot.plot(
            ax=ax, marker='o', linewidth=2, color=color_list[:len(top_types)]
        )

        plt.title('Evolution of Vulnerability Types Over Time')
        plt.xlabel('Month-Year')
        plt.ylabel('Count')
        plt.grid(True, linestyle='--', alpha=0.7)

        # Format x-axis as dates, rotate for readability
        import matplotlib.dates as mdates
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        plt.xticks(rotation=45, ha='right')

        # Legend for only the top types
        ax.legend(title="Top Vulnerability Types", bbox_to_anchor=(1.05, 1), loc='upper left')

        plt.tight_layout()
        
        # Use our new helper function to generate the filename
        filename = self.get_output_filename("temporal_vulnerability_trends", months)
        plt.savefig(f"{self.output_dir}/{filename}.png")
        
        # Calculate month-over-month growth rates for each vulnerability type
        growth_rates = pd.DataFrame()
        for vuln_type in monthly_trends.columns:
            monthly_data = monthly_trends[vuln_type]
            growth = monthly_data.pct_change() * 100  # Convert to percentage
            growth_rates[vuln_type] = growth
        
        # Use our new helper function to generate the filenames
        trends_filename = self.get_output_filename("vulnerability_trends_by_month", months)
        growth_filename = self.get_output_filename("vulnerability_growth_rates_monthly", months)
        
        monthly_trends.to_csv(f"{self.output_dir}/{trends_filename}.csv")
        growth_rates.to_csv(f"{self.output_dir}/{growth_filename}.csv")
        
        return monthly_trends, growth_rates

    def run_all_analyses(self, months=None, vendor_filter=None, product_filter=None):
        """
        Run all analyses and return results.
        
        Args:
            months (int, optional): Number of months to look back for temporal analysis.
            vendor_filter: String or list of strings to filter vendors
            product_filter: String or list of strings to filter products
        """
        # Apply filters if specified
        if vendor_filter or product_filter:
            self.filter_by_vendor_product(vendor_filter, product_filter)
        
        results = {}
        
        # Run basic analysis first
        print("\n1. Running basic vulnerability landscape analysis...")
        results['basic'] = self.analyze_vulnerability_landscape()
        
        # Run the six advanced analyses
        print("\n2. Analyzing temporal vulnerability trends...")
        results['temporal'] = self.analyze_temporal_trends(months=months)
        
        print("\n3. Analyzing vulnerability complexity...")
        results['complexity'] = self.analyze_complexity()
        
        print("\n4. Creating risk prioritization matrix...")
        results['risk_matrix'] = self.create_risk_matrix()
        
        print("\n5. Analyzing attack chains...")
        results['attack_chains'] = self.analyze_attack_chains()
        
        print("\n6. Analyzing mitigation coverage...")
        results['mitigation'] = self.analyze_mitigation_coverage()
        
        print("\n7. Analyzing zero-day windows...")
        results['zero_day'] = self.analyze_zero_day_windows()
        
        print(f"\nAll analyses complete. Reports saved to {self.output_dir}/")
        return results

    def set_exclude_types(self, exclude_types=None):
        """
        Update which vulnerability types to exclude from analysis.
        
        Args:
            exclude_types: List of vulnerability types to exclude or None to reset to default
        """
        if exclude_types is None:
            self.exclude_types = ["Unknown", "Other"]
        else:
            self.exclude_types = exclude_types
        
        # Update filtered dataframe - Create a proper copy to avoid SettingWithCopyWarning
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        print(f"Updated exclusion list. Now excluding: {self.exclude_types}")
        print(f"Filtered dataset contains {len(self.filtered_df)} records")

    def _smart_text_match(self, series, patterns):
        """
        Perform smart text matching on a pandas Series.
        
        Args:
            series: Pandas Series containing text to match against
            patterns: String or list of strings to match (case-insensitive partial match)
        
        Returns:
            Boolean Series indicating matches
        """
        # Convert patterns to list if it's a string
        if isinstance(patterns, str):
            patterns = [patterns]
        
        # Initialize result array with False values
        matches = pd.Series(False, index=series.index)
        
        # Convert series to lowercase strings
        series_lower = series.fillna('').astype(str).str.lower()
        
        # Check each pattern
        for pattern in patterns:
            pattern_lower = pattern.lower()
            # Match if pattern is contained in the text
            pattern_matches = series_lower.str.contains(pattern_lower, na=False)
            # Combine with OR logic (match any pattern)
            matches = matches | pattern_matches
        
        return matches

    def get_output_filename(self, base_name, months=None):
        """
        Generate an output filename that includes filter information.
        
        Args:
            base_name: Base filename without extension
            months: Optional month filter to include in filename
            
        Returns:
            Enhanced filename with filter information
        """
        filename = base_name
        
        # Add months info if applicable
        if months:
            filename += f"_{months}mo"
        
        # Add filter description if applicable
        if hasattr(self, 'filter_description') and self.filter_description:
            # Clean up the filter description for use in a filename
            clean_desc = self.filter_description.replace('=', '_').replace(',', '-')
            filename += f"_{clean_desc}"
        
        return filename


    def analyze_vulnerability_landscape(self):
        """Analyze the vulnerability landscape including prevalence and severity."""
        print("Analyzing vulnerability landscape...")
        
        # Define severity weights mapping with better null handling
        severity_weights = {
            "LOW": 1, "Low": 1, "low": 1,
            "MEDIUM": 3, "Medium": 3, "medium": 3,
            "HIGH": 6, "High": 6, "high": 6,
            "CRITICAL": 10, "Critical": 10, "critical": 10
        }
        
        # Filter the dataframe if exclusions are specified
        if self.exclude_types:
            filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)]
            print(f"Excluded {len(self.df) - len(filtered_df)} records with types: {self.exclude_types}")
        else:
            filtered_df = self.df
        
        # Vulnerability type distribution (prevalence)
        plt.figure(figsize=(14, 8))
        type_counts = filtered_df["vuln_type"].value_counts()
        
        # Plot the bar chart
        sns.barplot(y=type_counts.index, x=type_counts.values, color="steelblue")
        plt.title("Distribution of Vulnerability Types (Prevalence)")
        plt.xlabel("Count")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        
        # Use filter information in filename
        filename = self.get_output_filename("vuln_type_distribution")
        plt.savefig(f"{self.output_dir}/{filename}.png")
        
        # Create a dataframe with type, count and weighted score
        risk_scores = []
        for vuln_type in filtered_df["vuln_type"].unique():
            type_df = filtered_df[filtered_df["vuln_type"] == vuln_type]
            count = len(type_df)
            
            # Calculate severity score with better robustness
            if "severity" in type_df.columns:
                # Convert to string and map to weights with robust error handling
                valid_severities = type_df["severity"].dropna().astype(str)
                if len(valid_severities) > 0:
                    # Map severities to weights, defaulting to 0 for unknown values
                    severity_values = valid_severities.map(
                        lambda x: severity_weights.get(x, 0)
                    ).fillna(0)
                    
                    # Calculate average severity weight (if there are valid values)
                    if len(severity_values) > 0 and severity_values.sum() > 0:
                        weighted_score = severity_values.sum() / len(severity_values)
                    else:
                        weighted_score = 0
                else:
                    weighted_score = 0
            else:
                # If no severity column, use base_score as a proxy if available
                if "base_score" in type_df.columns:
                    valid_scores = type_df["base_score"].dropna()
                    if len(valid_scores) > 0:
                        # Normalize base score (0-10) to our weight scale (1-10)
                        weighted_score = valid_scores.mean() * (10/10)
                    else:
                        weighted_score = 0
                else:
                    weighted_score = 0
            
            # Risk score is count * severity weight
            risk_score = count * weighted_score
            
            risk_scores.append({
                "vuln_type": vuln_type,
                "count": count,
                "avg_severity_weight": weighted_score,
                "risk_score": risk_score
            })
        
        risk_df = pd.DataFrame(risk_scores).sort_values("risk_score", ascending=False)
        
        # Plot combined risk score
        plt.figure(figsize=(14, 8))
        sns.barplot(y="vuln_type", x="risk_score", data=risk_df, color="purple")
        plt.title("Combined Risk Score by Vulnerability Type (Prevalence × Severity)")
        plt.xlabel("Risk Score")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        
        # Use filter information in filename  
        risk_filename = self.get_output_filename("vuln_type_risk_score")
        plt.savefig(f"{self.output_dir}/{risk_filename}.png")
        
        # Save results with filter information in filename
        results_filename = self.get_output_filename("vulnerability_risk_analysis")
        risk_df.to_csv(f"{self.output_dir}/{results_filename}.csv", index=False)
        print(f"Risk analysis saved to {self.output_dir}/{results_filename}.csv")
        
        print("\nTop 5 vulnerability types by combined risk score:")
        print(risk_df[["vuln_type", "count", "avg_severity_weight", "risk_score"]].head(5))
        
        return risk_df

if __name__ == "__main__":
    # Parse command line arguments if any
    import argparse
    
    parser = argparse.ArgumentParser(description='TWX Advanced Vulnerability Analysis')
    parser.add_argument('--data', type=str, default="analysis/classification_data.json",
                  help='Path to the classified vulnerability data (JSON or CSV)')
    parser.add_argument('--analyses', type=str, default="all",
                      help='Comma-separated list of analyses to run, or "all"')
    parser.add_argument('--exclude', type=str, default=None,
                      help='Comma-separated list of vulnerability types to exclude')
    parser.add_argument('--output', type=str, default="analysis/reports",
                      help='Directory where reports will be saved')
    parser.add_argument('--months', type=int, default=None,
                      help='Number of months to look back for temporal analysis')
    parser.add_argument('--vendor', type=str, default=None,
                      help='Vendor filter (comma-separated for multiple patterns)')
    parser.add_argument('--product', type=str, default=None,
                      help='Product filter (comma-separated for multiple patterns)')
    
    args = parser.parse_args()
    
    # Process vendor and product filters
    vendor_filter = None
    if args.vendor:
        vendor_filter = [v.strip() for v in args.vendor.split(',')]
    
    product_filter = None
    if args.product:
        product_filter = [p.strip() for p in args.product.split(',')]
    
    try:
        # Initialize the analyzer with data path only
        analyzer = VulnerabilityAnalyzer(data_path=args.data)
        
        # Apply filters if specified (using the dedicated method)
        if vendor_filter or product_filter:
            analyzer.filter_by_vendor_product(vendor_filter, product_filter)
        
        # Set output directory if specified
        if args.output:
            analyzer.output_dir = args.output
            os.makedirs(analyzer.output_dir, exist_ok=True)
        
        # Set exclusion types if specified
        if args.exclude:
            exclude_list = [t.strip() for t in args.exclude.split(',')]
            analyzer.set_exclude_types(exclude_list)
        
        print(f"Ready to analyze {len(analyzer.filtered_df)} vulnerability records")
        
        # Run analyses based on arguments
        if args.analyses.lower() == "all":
            # Run all analyses, passing the months parameter to temporal trends
            analyzer.analyze_vulnerability_landscape()
            analyzer.analyze_temporal_trends(months=args.months)
            analyzer.analyze_complexity()
            analyzer.create_risk_matrix()
            analyzer.analyze_attack_chains()
            analyzer.analyze_mitigation_coverage()
            analyzer.analyze_zero_day_windows()
        else:
            # Run selected analyses
            analyses = args.analyses.split(',')
            for analysis in analyses:
                analysis = analysis.strip().lower()
                if analysis == "basic":
                    analyzer.analyze_vulnerability_landscape()
                elif analysis == "temporal":
                    analyzer.analyze_temporal_trends(months=args.months)
                elif analysis == "complexity":
                    analyzer.analyze_complexity()
                elif analysis == "risk":
                    analyzer.create_risk_matrix()
                elif analysis == "attack_chains":
                    analyzer.analyze_attack_chains()
                elif analysis == "mitigation":
                    analyzer.analyze_mitigation_coverage()
                elif analysis == "zero_day":
                    analyzer.analyze_zero_day_windows()
                else:
                    print(f"Warning: Unknown analysis type '{analysis}'")
    
    except Exception as e:
        print(f"Error running analysis: {e}")
        import traceback
        traceback.print_exc()