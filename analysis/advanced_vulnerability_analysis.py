"""
Advanced Vulnerability Analysis Module for TWX

This module provides comprehensive analysis capabilities for vulnerability data.
It works independently from the classifier module and provides multiple analysis types.
"""
import re
import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import Dict, Optional
import networkx as nx
from collections import defaultdict
import json

# Add the project root directory to Python path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(root_dir)

from storage.vulnerability_db import VulnerabilityDatabase


class VulnerabilityAnalyzer:
    """Advanced vulnerability data analyzer with multiple analysis capabilities."""
    
    def __init__(self, data_path="analysis/classification_data.csv", 
                 vendor_filter=None, product_filter=None):
        """Initialize the analyzer with vulnerability data from JSON."""
        # Check if file exists and is JSON
        if os.path.exists(data_path) and data_path.lower().endswith('.json'):
            # Load from JSON
            self.df = pd.read_json(data_path, orient='records')
            print(f"Loaded {len(self.df)} vulnerability records from JSON")
        else:
            # Fall back to CSV if JSON not found or for backward compatibility
            csv_path = data_path.replace('.json', '.csv') if data_path.lower().endswith('.json') else data_path
            if os.path.exists(csv_path):
                self.df = pd.read_csv(csv_path, low_memory=False, parse_dates=['published', 'modified'])
                print(f"Loaded {len(self.df)} vulnerability records from CSV (fallback)")
            else:
                raise FileNotFoundError(f"Could not find data at {data_path} or {csv_path}")
        
        if "cvss_vector" in self.df.columns:
            self.df = parse_cvss_vector_fields(self.df, vector_col="cvss_vector")
    
        # Convert date columns
        for date_col in ['published', 'modified', 'patch_date']:
            if date_col in self.df.columns:
                self.df[date_col] = pd.to_datetime(self.df[date_col], errors='coerce')
        # Create vuln_type if it doesn't exist in the data
        if "vuln_type" not in self.df.columns:
            print("'vuln_type' column not found - creating from CWE data")

            # Map CWE to vulnerability types using our categorization
            cwe_type_map = {
                # Memory Safety
                "CWE-119": "Buffer Overflow",
                "CWE-120": "Buffer Overflow", 
                "CWE-125": "Buffer Overflow",
                "CWE-787": "Buffer Overflow",
                "CWE-416": "Use After Free",
                "CWE-415": "Double Free",
                "CWE-476": "Null Pointer Dereference",
                
                # Injection
                "CWE-89": "SQL Injection",
                "CWE-564": "SQL Injection",
                "CWE-77": "Command Injection",
                "CWE-78": "Command Injection",
                "CWE-79": "Cross-site Scripting",
                "CWE-80": "Cross-site Scripting",
                "CWE-83": "Cross-site Scripting",
                "CWE-91": "XML Injection",
                "CWE-643": "XML Injection",
                "CWE-90": "LDAP Injection",
                "CWE-134": "Format String",
                
                # Access Control
                "CWE-284": "Access Control",
                "CWE-285": "Access Control",
                "CWE-287": "Authentication Issues",
                "CWE-306": "Authentication Issues",
                "CWE-22": "Path Traversal",
                "CWE-23": "Path Traversal",
                "CWE-36": "Path Traversal",
                "CWE-269": "Privilege Escalation",
                "CWE-732": "Incorrect Permissions",
                
                # Other common types
                "CWE-200": "Information Disclosure",
                "CWE-20": "Input Validation", 
                "CWE-352": "Cross-Site Request Forgery",
                "CWE-434": "Unrestricted File Upload",
                "CWE-94": "Code Injection",
                "CWE-400": "Resource Management"
            }

            # Define map_cwe_to_type in the outer scope so it's always available
            def map_cwe_to_type(cwe):
                if pd.isna(cwe) or not cwe:
                    return "Unknown"
                
                if cwe in cwe_type_map:
                    return cwe_type_map[cwe]
                    
                # Fallbacks based on CWE number ranges 
                if cwe.startswith("CWE-"):
                    try:
                        cwe_num = int(cwe[4:])
                        
                        # Use number ranges for common categories
                        if 119 <= cwe_num <= 127:  # Memory buffer errors
                            return "Buffer Overflow"
                        elif 74 <= cwe_num <= 94:   # Injection issues
                            return "Injection"
                        elif 264 <= cwe_num <= 286:  # Access/permissions issues
                            return "Access Control"
                        elif 310 <= cwe_num <= 340:  # Crypto issues
                            return "Cryptographic Issues"
                        elif 200 <= cwe_num <= 213:  # Information disclosure
                            return "Information Disclosure"
                    except ValueError:
                        pass
                
                return "Other"

            if "primary_cwe" in self.df.columns:
                self.df["vuln_type"] = self.df["primary_cwe"].apply(map_cwe_to_type)
            elif "cwe_ids" in self.df.columns:
                # If we have a list of CWEs but no primary_cwe, extract the first one
                def extract_first_cwe(cwe_str):
                    if pd.isna(cwe_str) or not cwe_str:
                        return "Unknown"
                        
                    # Parse the string representation of CWE list
                    if isinstance(cwe_str, str):
                        # Handle formats like "['CWE-79', 'CWE-89']" or "CWE-79,CWE-89"
                        if ',' in cwe_str:
                            # Extract first CWE from comma-separated list
                            cwe_list = [cwe.strip().strip("'[]\"") for cwe in cwe_str.split(',')]
                            if cwe_list:
                                first_cwe = cwe_list[0]
                                if 'CWE-' in first_cwe:
                                    return first_cwe
                        elif 'CWE-' in cwe_str:
                            # Direct CWE ID 
                            return cwe_str.strip().strip("'[]\"")
                    
                    return "Unknown"
                    
                # Extract first CWE and then map to type
                self.df["primary_cwe"] = self.df["cwe_ids"].apply(extract_first_cwe)
                self.df["vuln_type"] = self.df["primary_cwe"].apply(map_cwe_to_type)
            else:
                # If we have no CWE data, set Unknown
                self.df["vuln_type"] = "Unknown"
        
        # Filter out Unknown and Other by default
        self.exclude_types = ["Unknown", "Other"]
        
        # Create a proper copy to avoid SettingWithCopyWarning
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        
        # Initialize filter description (for output filenames)
        self.filter_description = None
        
        # Apply vendor/product filters if specified
        if vendor_filter or product_filter:
            self.filter_by_vendor_product(vendor_filter, product_filter)
        
        # Define output directory
        self.output_dir = "analysis/reports"
        os.makedirs(self.output_dir, exist_ok=True)

 
    def analyze_attack_graphs(self, max_depth=5, min_edge_weight=0.1):
        """
        Build and analyze attack graphs showing how vulnerabilities can be chained together.
        
        This analysis identifies potential attack paths by examining:
        - How vulnerabilities of different types commonly co-occur on the same products
        - Which vulnerability types enable other types (e.g., info disclosure → authentication bypass)
        - The most critical attack paths based on severity and exploitability
        
        Args:
            max_depth: Maximum depth of attack paths to explore
            min_edge_weight: Minimum weight for edges to include in the graph
            
        Returns:
            Tuple of (attack_graph, critical_paths, path_statistics)
        """
        print("Building attack graph from vulnerability relationships...")
        
        # Create a directed graph
        G = nx.DiGraph()
        
        # Define attack phase progression (based on MITRE ATT&CK kill chain)
        attack_progression = {
            # Initial Access vulnerabilities
            'Information Disclosure': ['Authentication Issues', 'Access Control', 'Command Injection'],
            'Input Validation': ['Cross-site Scripting', 'SQL Injection', 'Command Injection'],
            'Open Redirect': ['Cross-site Scripting', 'Authentication Issues'],
            
            # Execution phase
            'Cross-site Scripting': ['Session Fixation', 'Privilege Escalation', 'Information Disclosure'],
            'SQL Injection': ['Information Disclosure', 'Authentication Issues', 'Privilege Escalation'],
            'Command Injection': ['Privilege Escalation', 'Code Injection', 'File Upload'],
            'Code Injection': ['Privilege Escalation', 'Remote Code Execution'],
            
            # Persistence & Privilege Escalation
            'Authentication Issues': ['Privilege Escalation', 'Access Control', 'Session Management'],
            'Access Control': ['Privilege Escalation', 'Information Disclosure', 'Data Manipulation'],
            'Privilege Escalation': ['Remote Code Execution', 'Information Disclosure', 'Persistence'],
            
            # Lateral Movement & Collection
            'Path Traversal': ['Information Disclosure', 'File Upload', 'Remote File Inclusion'],
            'SSRF': ['Information Disclosure', 'Access Control', 'Remote Code Execution'],
            'Deserialization': ['Remote Code Execution', 'Privilege Escalation'],
            
            # Defense Evasion & Exfiltration
            'Cryptographic Issues': ['Information Disclosure', 'Authentication Issues'],
            'Logic Errors': ['Access Control', 'Authentication Issues', 'Information Disclosure']
        }
        
        # Build nodes for each vulnerability type with statistics
        vuln_type_stats = self.filtered_df.groupby('vuln_type').agg({
            'base_score': ['mean', 'max'],
            'exploitability_score': 'mean',
            'has_exploit': 'sum',
            'vuln_id': 'count'
        }).round(2)
        
        # Add nodes with attributes
        for vuln_type, stats in vuln_type_stats.iterrows():
            G.add_node(vuln_type, 
                    count=stats[('vuln_id', 'count')],
                    avg_severity=stats[('base_score', 'mean')],
                    max_severity=stats[('base_score', 'max')],
                    avg_exploitability=stats[('exploitability_score', 'mean')],
                    exploits_available=stats[('has_exploit', 'sum')],
                    risk_score=stats[('vuln_id', 'count')] * stats[('base_score', 'mean')])
        
        # Build edges based on co-occurrence and logical progression
        # First, add edges from the predefined attack progression
        for source, targets in attack_progression.items():
            if source in G.nodes():
                for target in targets:
                    if target in G.nodes():
                        # Weight based on how often this progression occurs
                        weight = self._calculate_progression_weight(source, target)
                        if weight >= min_edge_weight:
                            G.add_edge(source, target, weight=weight, type='progression')
        
        # Second, add edges based on actual co-occurrence in products
        co_occurrence_edges = self._find_vulnerability_co_occurrences()
        for (source, target), weight in co_occurrence_edges.items():
            if source in G.nodes() and target in G.nodes() and weight >= min_edge_weight:
                if G.has_edge(source, target):
                    # Strengthen existing edge
                    G[source][target]['weight'] = min(1.0, G[source][target]['weight'] + weight * 0.5)
                else:
                    G.add_edge(source, target, weight=weight, type='co_occurrence')
        
        # Find critical attack paths
        critical_paths = self._find_critical_attack_paths(G, max_depth)
        
        # Calculate path statistics
        path_stats = self._analyze_path_statistics(G, critical_paths)
        
        # Visualize the attack graph
        self._visualize_attack_graph(G, critical_paths)
        
        # Save graph data
        self._save_attack_graph_data(G, critical_paths, path_stats)
        
        return G, critical_paths, path_stats
    
    def _calculate_progression_weight(self, source_type, target_type):
            """Calculate the weight of progression from one vuln type to another."""
            # Check how often these vulnerability types appear together on the same products
            source_products = set(self.filtered_df[self.filtered_df['vuln_type'] == source_type]['products'].dropna())
            target_products = set(self.filtered_df[self.filtered_df['vuln_type'] == target_type]['products'].dropna())
            
            if not source_products or not target_products:
                return 0.0
            
            # Jaccard similarity
            intersection = len(source_products & target_products)
            union = len(source_products | target_products)
            
            if union == 0:
                return 0.0
            
            return intersection / union
        
    def _find_vulnerability_co_occurrences(self):
            """Find which vulnerability types commonly occur together on the same products."""
            co_occurrences = defaultdict(int)
            
            # Group by product to find co-occurring vulnerabilities
            if 'products' in self.filtered_df.columns and 'vendors' in self.filtered_df.columns:
                product_vulns = self.filtered_df.groupby(['vendors', 'products'])['vuln_type'].apply(list)
                
                for product, vuln_types in product_vulns.items():
                    # Get unique vulnerability types for this product
                    unique_types = list(set(vuln_types))
                    
                    # Count co-occurrences
                    for i, type1 in enumerate(unique_types):
                        for type2 in unique_types[i+1:]:
                            # Order consistently
                            pair = tuple(sorted([type1, type2]))
                            co_occurrences[pair] += 1
            
            # Normalize weights
            max_count = max(co_occurrences.values()) if co_occurrences else 1
            normalized = {pair: count / max_count for pair, count in co_occurrences.items()}
            
            return normalized
        
    def _find_critical_attack_paths(self, G, max_depth):
            """Identify the most critical attack paths in the graph."""
            critical_paths = []
            
            # Identify entry points (nodes with low in-degree but high severity)
            entry_points = []
            for node in G.nodes():
                in_degree = G.in_degree(node)
                if in_degree <= 2:  # Potential entry point
                    severity = G.nodes[node].get('avg_severity', 0)
                    if severity >= 5.0:  # Medium severity or higher
                        entry_points.append(node)
            
            # Identify critical targets (high-value nodes)
            critical_targets = []
            for node in G.nodes():
                risk_score = G.nodes[node].get('risk_score', 0)
                if risk_score >= np.percentile([G.nodes[n].get('risk_score', 0) for n in G.nodes()], 75):
                    critical_targets.append(node)
            
            # Find paths from entry points to critical targets
            for entry in entry_points:
                for target in critical_targets:
                    if entry != target:
                        try:
                            # Find all simple paths up to max_depth
                            paths = list(nx.all_simple_paths(G, entry, target, cutoff=max_depth))
                            for path in paths:
                                # Calculate path score
                                path_score = self._calculate_path_score(G, path)
                                critical_paths.append({
                                    'path': path,
                                    'score': path_score,
                                    'entry': entry,
                                    'target': target,
                                    'length': len(path)
                                })
                        except nx.NetworkXNoPath:
                            continue
            
            # Sort by score and return top paths
            critical_paths.sort(key=lambda x: x['score'], reverse=True)
            return critical_paths[:20]  # Top 20 paths
        
    def _calculate_path_score(self, G, path):
            """Calculate the criticality score of an attack path."""
            if len(path) < 2:
                return 0
            
            # Factors: path length (shorter is more critical), node severity, edge weights
            path_score = 0
            
            # Node scores
            for node in path:
                node_data = G.nodes[node]
                node_score = (
                    node_data.get('avg_severity', 0) * 0.4 +
                    node_data.get('avg_exploitability', 0) * 0.3 +
                    (node_data.get('exploits_available', 0) / max(1, node_data.get('count', 1))) * 10 * 0.3
                )
                path_score += node_score
            
            # Edge weights
            for i in range(len(path) - 1):
                if G.has_edge(path[i], path[i+1]):
                    edge_weight = G[path[i]][path[i+1]].get('weight', 0)
                    path_score += edge_weight * 5
            
            # Length penalty (shorter paths are more dangerous)
            length_penalty = 1.0 / (1 + np.log(len(path)))
            path_score *= length_penalty
            
            return path_score
        
    def _analyze_path_statistics(self, G, critical_paths):
            """Analyze statistics about the attack paths."""
            stats = {
                'total_nodes': len(G.nodes()),
                'total_edges': len(G.edges()),
                'avg_degree': sum(dict(G.degree()).values()) / len(G.nodes()),
                'strongly_connected_components': nx.number_strongly_connected_components(G),
                'critical_paths_found': len(critical_paths),
                'avg_path_length': np.mean([p['length'] for p in critical_paths]) if critical_paths else 0,
                'most_common_entry_points': {},
                'most_common_targets': {},
                'most_traversed_nodes': {}
            }
            
            # Analyze critical paths
            if critical_paths:
                entry_counts = defaultdict(int)
                target_counts = defaultdict(int)
                node_traversals = defaultdict(int)
                
                for path_info in critical_paths:
                    entry_counts[path_info['entry']] += 1
                    target_counts[path_info['target']] += 1
                    for node in path_info['path']:
                        node_traversals[node] += 1
                
                # Top 5 for each category
                stats['most_common_entry_points'] = dict(sorted(entry_counts.items(), 
                                                            key=lambda x: x[1], reverse=True)[:5])
                stats['most_common_targets'] = dict(sorted(target_counts.items(), 
                                                        key=lambda x: x[1], reverse=True)[:5])
                stats['most_traversed_nodes'] = dict(sorted(node_traversals.items(), 
                                                        key=lambda x: x[1], reverse=True)[:5])
            
            return stats
        
    def _visualize_attack_graph(self, G, critical_paths):
            """Create visualizations of the attack graph."""
            # Main attack graph visualization
            plt.figure(figsize=(20, 16))
            
            # Create layout
            pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
            
            # Node colors based on risk score
            node_colors = []
            node_sizes = []
            for node in G.nodes():
                risk_score = G.nodes[node].get('risk_score', 0)
                # Normalize risk score for coloring
                normalized_risk = min(risk_score / 1000, 1.0)  # Cap at 1.0
                node_colors.append(plt.cm.YlOrRd(normalized_risk))
                # Size based on count
                node_sizes.append(100 + G.nodes[node].get('count', 0) * 2)
            
            # Draw the graph
            nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
            
            # Draw edges with different styles
            progression_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('type') == 'progression']
            co_occurrence_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('type') == 'co_occurrence']
            
            nx.draw_networkx_edges(G, pos, edgelist=progression_edges, edge_color='blue', 
                                alpha=0.5, width=2, style='solid', arrows=True, arrowsize=20)
            nx.draw_networkx_edges(G, pos, edgelist=co_occurrence_edges, edge_color='green', 
                                alpha=0.3, width=1, style='dashed', arrows=True, arrowsize=15)
            
            # Draw labels
            labels = {}
            for node in G.nodes():
                if G.degree(node) >= 3 or G.nodes[node].get('risk_score', 0) > 500:
                    # Only label important nodes
                    labels[node] = node
            
            nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold')
            
            plt.title('Vulnerability Attack Graph', fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()
            
            # Add legend
            from matplotlib.lines import Line2D
            legend_elements = [
                Line2D([0], [0], color='blue', lw=2, label='Attack Progression'),
                Line2D([0], [0], color='green', lw=2, linestyle='--', label='Co-occurrence'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', 
                    markersize=10, label='Low Risk'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='red', 
                    markersize=10, label='High Risk')
            ]
            plt.legend(handles=legend_elements, loc='upper right', fontsize=12)
            
            # Save the main graph
            graph_filename = self.get_output_filename("attack_graph")
            plt.savefig(f"{self.output_dir}/{graph_filename}.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # Create a focused view of critical paths
            if critical_paths:
                self._visualize_critical_paths(G, critical_paths[:5], pos)
        
    def _visualize_critical_paths(self, G, top_paths, pos):
            """Visualize the top critical attack paths."""
            plt.figure(figsize=(16, 10))
            
            # Create subgraph with only nodes in critical paths
            critical_nodes = set()
            for path_info in top_paths:
                critical_nodes.update(path_info['path'])
            
            # Draw the full graph in light gray
            nx.draw_networkx_nodes(G, pos, node_color='lightgray', node_size=100, alpha=0.3)
            nx.draw_networkx_edges(G, pos, edge_color='lightgray', alpha=0.2, width=0.5)
            
            # Highlight critical paths with different colors
            colors = plt.cm.Set1(np.linspace(0, 1, len(top_paths)))
            
            for i, path_info in enumerate(top_paths):
                path = path_info['path']
                path_edges = [(path[j], path[j+1]) for j in range(len(path)-1)]
                
                # Draw path nodes
                nx.draw_networkx_nodes(G, pos, nodelist=path, node_color=[colors[i]], 
                                    node_size=300, alpha=0.8)
                
                # Draw path edges
                nx.draw_networkx_edges(G, pos, edgelist=path_edges, edge_color=[colors[i]], 
                                    width=3, alpha=0.8, arrows=True, arrowsize=20)
                
                # Label entry and target
                nx.draw_networkx_labels(G, pos, {path[0]: f"Entry {i+1}", path[-1]: f"Target {i+1}"}, 
                                    font_size=12, font_weight='bold')
            
            plt.title('Top 5 Critical Attack Paths', fontsize=16, fontweight='bold')
            plt.axis('off')
            
            # Add path legend
            legend_text = []
            for i, path_info in enumerate(top_paths):
                path_str = " → ".join(path_info['path'][:3])
                if len(path_info['path']) > 3:
                    path_str += f" → ... ({len(path_info['path'])} steps)"
                legend_text.append(f"Path {i+1} (score: {path_info['score']:.2f}): {path_str}")
            
            plt.text(0.02, 0.98, '\n'.join(legend_text), transform=plt.gca().transAxes,
                    fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', 
                    facecolor='wheat', alpha=0.8))
            
            plt.tight_layout()
            
            # Save critical paths visualization
            paths_filename = self.get_output_filename("critical_attack_paths")
            plt.savefig(f"{self.output_dir}/{paths_filename}.png", dpi=300, bbox_inches='tight')
            plt.close()
        
    def _save_attack_graph_data(self, G, critical_paths, path_stats):
            """Save attack graph data for further analysis."""
            # Save graph structure
            graph_data = {
                'nodes': [],
                'edges': [],
                'critical_paths': [],
                'statistics': path_stats
            }
            
            # Node data
            for node in G.nodes():
                node_data = G.nodes[node].copy()
                node_data['id'] = node
                node_data['in_degree'] = G.in_degree(node)
                node_data['out_degree'] = G.out_degree(node)
                graph_data['nodes'].append(node_data)
            
            # Edge data
            for source, target, data in G.edges(data=True):
                edge_data = data.copy()
                edge_data['source'] = source
                edge_data['target'] = target
                graph_data['edges'].append(edge_data)
            
            # Critical paths
            for path_info in critical_paths[:20]:  # Top 20 paths
                graph_data['critical_paths'].append({
                    'path': path_info['path'],
                    'score': float(path_info['score']),
                    'entry': path_info['entry'],
                    'target': path_info['target'],
                    'length': path_info['length']
                })
            
            # Save as JSON
            json_filename = self.get_output_filename("attack_graph_data")
            with open(f"{self.output_dir}/{json_filename}.json", 'w') as f:
                json.dump(graph_data, f, indent=2)
            
            # Save summary report
            self._generate_attack_graph_report(G, critical_paths, path_stats)
        
    def _generate_attack_graph_report(self, G, critical_paths, path_stats):
            """Generate a human-readable report of the attack graph analysis."""
            report_lines = [
                "=" * 80,
                "ATTACK GRAPH ANALYSIS REPORT",
                "=" * 80,
                f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "",
                "GRAPH OVERVIEW",
                "-" * 40,
                f"Total vulnerability types (nodes): {path_stats['total_nodes']}",
                f"Total attack relationships (edges): {path_stats['total_edges']}",
                f"Average connections per vulnerability: {path_stats['avg_degree']:.2f}",
                f"Strongly connected components: {path_stats['strongly_connected_components']}",
                "",
                "CRITICAL ATTACK PATHS",
                "-" * 40,
                f"Critical paths identified: {path_stats['critical_paths_found']}",
                f"Average path length: {path_stats['avg_path_length']:.2f}",
                "",
                "TOP ENTRY POINTS (Initial Compromise Vectors):",
            ]
            
            for vuln_type, count in path_stats['most_common_entry_points'].items():
                severity = G.nodes[vuln_type].get('avg_severity', 0)
                report_lines.append(f"  - {vuln_type}: {count} paths (avg severity: {severity:.1f})")
            
            report_lines.extend([
                "",
                "TOP TARGETS (Critical Vulnerabilities Reached):",
            ])
            
            for vuln_type, count in path_stats['most_common_targets'].items():
                risk_score = G.nodes[vuln_type].get('risk_score', 0)
                report_lines.append(f"  - {vuln_type}: {count} paths (risk score: {risk_score:.0f})")
            
            report_lines.extend([
                "",
                "MOST TRAVERSED VULNERABILITIES (Key Pivot Points):",
            ])
            
            for vuln_type, count in path_stats['most_traversed_nodes'].items():
                report_lines.append(f"  - {vuln_type}: traversed {count} times")
            
            report_lines.extend([
                "",
                "TOP 10 CRITICAL ATTACK PATHS:",
                "-" * 40,
            ])
            
            for i, path_info in enumerate(critical_paths[:10], 1):
                path_str = " → ".join(path_info['path'])
                report_lines.extend([
                    f"\n{i}. Score: {path_info['score']:.2f}",
                    f"   Path: {path_str}",
                    f"   Length: {path_info['length']} steps",
                ])
            
            report_lines.extend([
                "",
                "RECOMMENDATIONS:",
                "-" * 40,
                "1. Priority patching for entry point vulnerabilities:",
            ])
            
            # Get top 3 entry points
            for vuln_type, _ in list(path_stats['most_common_entry_points'].items())[:3]:
                count = G.nodes[vuln_type].get('count', 0)
                report_lines.append(f"   - {vuln_type} ({count} instances)")
            
            report_lines.extend([
                "",
                "2. Critical vulnerabilities requiring immediate attention:",
            ])
            
            # Get highest risk nodes
            high_risk_nodes = sorted(G.nodes(), 
                                key=lambda n: G.nodes[n].get('risk_score', 0), 
                                reverse=True)[:3]
            for node in high_risk_nodes:
                risk_score = G.nodes[node].get('risk_score', 0)
                report_lines.append(f"   - {node} (risk score: {risk_score:.0f})")
            
            report_lines.extend([
                "",
                "3. Key pivot points to monitor:",
            ])
            
            for vuln_type, _ in list(path_stats['most_traversed_nodes'].items())[:3]:
                report_lines.append(f"   - {vuln_type}")
            
            report_lines.extend([
                "",
                "=" * 80
            ])
            
            # Save report
            report_filename = self.get_output_filename("attack_graph_report")
            with open(f"{self.output_dir}/{report_filename}.txt", 'w') as f:
                f.write('\n'.join(report_lines))
            
            print(f"\nAttack graph report saved to {self.output_dir}/{report_filename}.txt")

    # ...existing code...
    def analyze_contextual_priorities(self, 
                                    asset_inventory: pd.DataFrame,
                                    business_criticality: Dict[str, float],
                                    network_exposure: Dict[str, str],
                                    operational_constraints: Optional[Dict] = None) -> pd.DataFrame:
        """
        Analyze vulnerabilities with context-aware prioritization based on specific environment.
        
        This is a simplified version that works with the existing analyzer infrastructure.
        
        Args:
            asset_inventory: DataFrame with columns [asset_id, vendor, product, version, asset_type]
            business_criticality: Dict mapping asset_id to criticality score (0-1)
            network_exposure: Dict mapping asset_id to exposure level ('internal', 'dmz', 'internet')
            operational_constraints: Optional dict with maintenance windows, patch testing requirements
            
        Returns:
            DataFrame with prioritized vulnerabilities including contextual risk scores
        """
        print("Analyzing contextual vulnerability priorities...")
        
        # Get vulnerabilities matching the asset inventory
        relevant_vulns = []
        
        for _, asset in asset_inventory.iterrows():
            # Filter vulnerabilities by vendor/product
            vendor_filter = asset['vendor'].lower()
            product_filter = asset['product'].lower()
            
            # Find matching vulnerabilities in our dataset
            vendor_matches = self.filtered_df['vendors'].fillna('').str.lower().str.contains(vendor_filter, na=False)
            product_matches = self.filtered_df['products'].fillna('').str.lower().str.contains(product_filter, na=False)
            
            matching_vulns = self.filtered_df[vendor_matches & product_matches].copy()
            matching_vulns['asset_id'] = asset['asset_id']
            relevant_vulns.append(matching_vulns)
        
        if not relevant_vulns:
            print("No vulnerabilities found matching the asset inventory")
            return pd.DataFrame()
        
        vuln_data = pd.concat(relevant_vulns, ignore_index=True)
        
        # Calculate base risk scores
        vuln_data['base_risk_score'] = vuln_data['base_score'].fillna(5.0)
        vuln_data.loc[vuln_data['known_exploited'] == True, 'base_risk_score'] *= 1.5
        vuln_data['base_risk_score'] = vuln_data['base_risk_score'].clip(0, 10)
        
        # Apply criticality multiplier
        asset_crit = pd.DataFrame([
            {'asset_id': k, 'criticality': v} 
            for k, v in business_criticality.items()
        ])
        vuln_data = vuln_data.merge(asset_crit, on='asset_id', how='left')
        vuln_data['criticality_multiplier'] = 0.5 + (vuln_data['criticality'].fillna(0.5) * 1.5)
        
        # Apply exposure multiplier
        exposure_scores = {
            'internal': 0.5,
            'dmz': 1.0,
            'internet': 2.0
        }
        asset_exposure = pd.DataFrame([
            {'asset_id': k, 'exposure': v} 
            for k, v in network_exposure.items()
        ])
        vuln_data = vuln_data.merge(asset_exposure, on='asset_id', how='left')
        vuln_data['exposure_multiplier'] = vuln_data['exposure'].map(exposure_scores).fillna(1.0)
        
        # Apply exploit availability multiplier
        vuln_data['exploit_multiplier'] = 1.0
        vuln_data.loc[vuln_data['known_exploited'] == True, 'exploit_multiplier'] = 2.0
        vuln_data.loc[vuln_data['has_exploit'] == True, 'exploit_multiplier'] = 1.5
        
        # Calculate final contextual risk score
        vuln_data['contextual_risk_score'] = (
            vuln_data['base_risk_score'] * 
            vuln_data['criticality_multiplier'] * 
            vuln_data['exposure_multiplier'] * 
            vuln_data['exploit_multiplier']
        )
        
        # Sort by contextual risk score
        vuln_data = vuln_data.sort_values('contextual_risk_score', ascending=False)
        
        # Add priority tiers
        if len(vuln_data) > 0:
            vuln_data['priority_tier'] = pd.qcut(
                vuln_data['contextual_risk_score'], 
                q=[0, 0.7, 0.9, 0.95, 1.0], 
                labels=['Low', 'Medium', 'High', 'Critical'],
                duplicates='drop'
            )
        
        # Create visualization
        if len(vuln_data) > 0:
            plt.figure(figsize=(15, 10))
            
            # Group by asset and priority tier
            priority_summary = vuln_data.groupby(['asset_id', 'priority_tier']).size().unstack(fill_value=0)
            
            # Create stacked bar chart
            priority_summary.plot(kind='bar', stacked=True, 
                                color=['green', 'yellow', 'orange', 'red'],
                                alpha=0.8)
            
            plt.title('Vulnerability Priorities by Asset')
            plt.xlabel('Asset ID')
            plt.ylabel('Number of Vulnerabilities')
            plt.legend(title='Priority Tier', bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()
            
            # Save plot
            plot_filename = self.get_output_filename("contextual_priorities")
            plt.savefig(f"{self.output_dir}/{plot_filename}.png")
            
            # Save results
            csv_filename = self.get_output_filename("contextual_vulnerability_priorities")
            vuln_data.to_csv(f"{self.output_dir}/{csv_filename}.csv", index=False)
            
            print(f"\nContextual prioritization complete:")
            print(f"- Total vulnerabilities analyzed: {len(vuln_data)}")
            print(f"- Critical priority: {len(vuln_data[vuln_data['priority_tier'] == 'Critical'])}")
            print(f"- High priority: {len(vuln_data[vuln_data['priority_tier'] == 'High'])}")
            print(f"- Results saved to {self.output_dir}/{csv_filename}.csv")
        
        return vuln_data

    # ...existing code...
    def analyze_risk_vs_complexity(self):
        """
        Combine risk and complexity analyses to identify vulnerability types that are both high risk and easy to exploit.
        Generates a CSV and a scatter plot.
        """
        # Use filter-aware filenames
        risk_filename = self.get_output_filename("vulnerability_risk_analysis") + ".csv"
        complexity_filename = self.get_output_filename("vulnerability_complexity") + ".csv"
        risk_path = os.path.join(self.output_dir, risk_filename)
        complexity_path = os.path.join(self.output_dir, complexity_filename)

        if not (os.path.exists(risk_path) and os.path.exists(complexity_path)):
            print(f"Risk or complexity analysis files not found. Expected:\n  {risk_path}\n  {complexity_path}\nRun those analyses first.")
            return None

        risk_df = pd.read_csv(risk_path)
        complexity_df = pd.read_csv(complexity_path)
        # Merge on vuln_type
        merged = pd.merge(risk_df, complexity_df, on="vuln_type", suffixes=('_risk', '_complexity'))

        # Filter for types with enough samples (optional)
        merged = merged[(merged['count_risk'] >= 10) & (merged['count_complexity'] >= 10)]

        # Rank by high risk and low complexity
        merged['risk_rank'] = merged['risk_score'].rank(ascending=False)
        merged['complexity_rank'] = merged['mean'].rank(ascending=True)  # lower is easier

        # Optionally, create a composite score (e.g., risk_score / (complexity_mean + 0.1))
        merged['priority_score'] = merged['risk_score'] / (merged['mean'] + 0.1)

        # Sort by priority
        merged = merged.sort_values('priority_score', ascending=False)

        # Save to CSV
        output_csv = os.path.join(self.output_dir, "risk_vs_complexity.csv")
        merged.to_csv(output_csv, index=False)
        print(f"Risk vs. Complexity analysis saved to {output_csv}")

        # Scatter plot
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(
            merged['mean'], merged['risk_score'],
            s=merged['count_risk'] * 0.5,  # scale bubble size
            c=merged['priority_score'], cmap='coolwarm', alpha=0.7, edgecolor='k'
        )
        for _, row in merged.iterrows():
            plt.text(row['mean'], row['risk_score'], row['vuln_type'], fontsize=8, ha='left', va='bottom')
        plt.xlabel("Average Exploitation Complexity (lower = easier)")
        plt.ylabel("Risk Score (prevalence × severity)")
        plt.title("Vulnerability Types: Risk vs. Exploitation Complexity")
        plt.colorbar(scatter, label="Priority Score")
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "risk_vs_complexity.png"))
        plt.show()

        return merged
    def filter_by_vendor_product(self, vendor_filter=None, product_filter=None):
        """
        Filter the dataset by vendor and/or product using smart text matching.
        
        Args:
            vendor_filter: String or list of strings to filter vendors (case-insensitive partial match)
            product_filter: String or list of strings to filter products (case-insensitive partial match)
        
        Returns:
            Number of records matching the filter criteria
        """
        # Start with the full dataset filtered by vulnerability type
        filtered_data = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        
        # Check if we have vendor and product columns
        if 'vendors' not in filtered_data.columns or 'products' not in filtered_data.columns:
            print("Warning: 'vendor' or 'product' columns missing, cannot apply filters")
            self.filtered_df = filtered_data
            return len(filtered_data)
        
        # Initial counts
        initial_count = len(filtered_data)
        
        # Apply vendor filter if specified
        if vendor_filter:
            vendor_matches = self._smart_text_match(filtered_data['vendors'], vendor_filter)
            filtered_data = filtered_data[vendor_matches].copy()
            print(f"Applied vendor filter: {len(filtered_data)} of {initial_count} records match")
        
        # Apply product filter if specified
        if product_filter:
            product_matches = self._smart_text_match(filtered_data['products'], product_filter)
            filtered_data = filtered_data[product_matches].copy()
            print(f"Applied product filter: {len(filtered_data)} of {initial_count} records match")
        
        # Update the filtered dataframe
        self.filtered_df = filtered_data
        
        # Generate filter description for reports
        filter_desc = []
        if vendor_filter:
            if isinstance(vendor_filter, list):
                filter_desc.append(f"vendors={','.join(vendor_filter)}")
            else:
                filter_desc.append(f"vendor={vendor_filter}")
        
        if product_filter:
            if isinstance(product_filter, list):
                filter_desc.append(f"products={','.join(product_filter)}")
            else:
                filter_desc.append(f"product={product_filter}")
        
        # Store filter description for file naming
        self.filter_description = "_".join(filter_desc) if filter_desc else None
        
        return len(filtered_data)

    def analyze_complexity(self):
        """Analyze relationship between CVSS metrics and vulnerability types."""
        print("Analyzing vulnerability complexity...")
        
        # Create complexity score based on Attack Complexity and User Interaction
        # Lower score means easier to exploit
        complexity_map = {'L': 0, 'H': 1}  # Low complexity = 0, High complexity = 1
        ui_map = {'N': 0, 'R': 1}  # No UI = 0, Required UI = 1
        
        # Ensure columns exist and handle missing values
        if 'ac' in self.filtered_df.columns and 'ui' in self.filtered_df.columns:
            valid_df = self.filtered_df[self.filtered_df['ac'].isin(['L', 'H']) & 
                                       self.filtered_df['ui'].isin(['N', 'R'])].copy()
            
            valid_df.loc[:, 'complexity_score'] = valid_df['ac'].map(complexity_map) + valid_df['ui'].map(ui_map)
            
            # Group by vulnerability type and calculate average complexity
            complexity_by_type = valid_df.groupby('vuln_type')['complexity_score'].agg(
                ['mean', 'count', 'std']
            ).sort_values('mean')
            
            # Only include types with sufficient samples
            min_samples = 5
            complexity_by_type = complexity_by_type[complexity_by_type['count'] >= min_samples]
            
            # Plot the results
            plt.figure(figsize=(14, 8))
            ax = complexity_by_type['mean'].plot(kind='barh', xerr=complexity_by_type['std'], 
                                    capsize=5, color='steelblue')
            plt.title('Average Exploitation Complexity by Vulnerability Type')
            plt.ylabel('Vulnerability Type')
            plt.xlabel('Complexity Score (lower is simpler to exploit)')
            plt.grid(True, linestyle='--', axis='x', alpha=0.7)
            
            # Add count annotations
            for i, (idx, row) in enumerate(complexity_by_type.iterrows()):
                plt.text(0.01, i, f"n={int(row['count'])}", va='center')
            
            plt.tight_layout()
            
            # Use filter information in filename
            complexity_filename = self.get_output_filename("vulnerability_complexity")
            plt.savefig(f"{self.output_dir}/{complexity_filename}.png")
            
            # Save to CSV with filter information
            csv_filename = self.get_output_filename("vulnerability_complexity")
            complexity_by_type.to_csv(f"{self.output_dir}/{csv_filename}.csv")
            
            return complexity_by_type
        else:
            print("Warning: 'ac' or 'ui' columns missing, skipping complexity analysis")
            return None
    
    def create_risk_matrix(self):
        """Create a risk prioritization matrix based on severity and exploitability."""
        print("Creating risk prioritization matrix...")
        
        # Check if needed columns exist
        required_cols = ['base_score', 'exploitability_score', 'has_exploit']
        if not all(col in self.filtered_df.columns for col in required_cols):
            print(f"Warning: One or more required columns missing: {required_cols}")
            print("Skipping risk matrix analysis")
            return None
        
        # Prepare data: severity, exploitability, whether exploit exists
        matrix_data = self.filtered_df[required_cols].copy()
        matrix_data = matrix_data.fillna(0)
        
        # Normalize data for clustering
        scaler = StandardScaler()
        matrix_data_scaled = scaler.fit_transform(matrix_data)
        
        # Apply K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        self.filtered_df.loc[:, 'risk_cluster'] = kmeans.fit_predict(matrix_data_scaled)
        
        # Assign cluster labels to each row
        self.filtered_df['risk_cluster'] = kmeans.labels_

        # Find the most common vuln_type in each cluster
        cluster_type_map = {}
        for i in range(4):
            types = self.filtered_df[self.filtered_df['risk_cluster'] == i]['vuln_type']
            if not types.empty:
                top_types = types.value_counts().head(2).index.tolist()
                cluster_type_map[i] = ", ".join(top_types)
            else:
                cluster_type_map[i] = "N/A"
        
        # Map clusters to risk levels
        cluster_centers = kmeans.cluster_centers_
        risk_importance = np.zeros(4)
        
        # Calculate risk importance for each cluster (higher is more risky)
        for i in range(4):
            # Base score is most important, then exploitability, then exploit existence
            risk_importance[i] = (cluster_centers[i, 0] * 0.5 +  # base_score (50% weight)
                                 cluster_centers[i, 1] * 0.3 +   # exploitability (30% weight)
                                 cluster_centers[i, 2] * 0.2)    # has_exploit (20% weight)
        
        # Sort clusters by risk importance (highest first)
        risk_mapping = {i: rank for rank, i in enumerate(np.argsort(risk_importance)[::-1])}
        risk_labels = ['Critical', 'High', 'Medium', 'Low']
        
        self.filtered_df.loc[:, 'risk_priority'] = self.filtered_df['risk_cluster'].map(
            {i: risk_labels[risk_mapping[i]] for i in range(4)}
        )
        
        # Plot the matrix
        plt.figure(figsize=(12, 10))
        
        # Convert has_exploit to numeric if needed
        if self.filtered_df['has_exploit'].dtype == bool:
            exploit_size = self.filtered_df['has_exploit'].astype(int) * 50 + 50
        else:
            exploit_size = self.filtered_df['has_exploit'].astype(float) * 50 + 50
        
        scatter = plt.scatter(
            self.filtered_df['base_score'], 
            self.filtered_df['exploitability_score'], 
            c=self.filtered_df['risk_cluster'], 
            cmap='viridis',
            s=exploit_size, 
            alpha=0.7
        )
        
        # Add colorbar legend
        cbar = plt.colorbar(scatter, label='Risk Cluster')
        cbar.set_ticks(range(4))
        cbar.set_ticklabels([f"{risk_labels[risk_mapping[i]]}" for i in range(4)])
        
        # Add cluster centers
        for i, (x, y) in enumerate(zip(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1])):
            plt.scatter(x, y, c='red', marker='X', s=200, label='Cluster Centers' if i == 0 else "")
            plt.text(x, y, f"{risk_labels[risk_mapping[i]]}\n({cluster_type_map[i]})",
                     fontsize=10, fontweight='bold', color='black', ha='center', va='bottom', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))
        
        plt.title('Risk Prioritization Matrix')
        plt.xlabel('CVSS Base Score')
        plt.ylabel('Exploitability Score')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        
        # Use filter information in filename
        matrix_filename = self.get_output_filename("risk_prioritization_matrix")
        plt.savefig(f"{self.output_dir}/{matrix_filename}.png")
        
        # Save risk distribution
        risk_dist = self.filtered_df['risk_priority'].value_counts().sort_index()
        risk_by_type = self.filtered_df.groupby(['vuln_type', 'risk_priority']).size().unstack().fillna(0)
        risk_by_type_pct = risk_by_type.div(risk_by_type.sum(axis=1), axis=0) * 100
        
        # Save to CSV with filter information in filenames
        priorities_filename = self.get_output_filename("vulnerability_risk_priorities")
        dist_filename = self.get_output_filename("risk_distribution_by_type") 
        pct_filename = self.get_output_filename("risk_distribution_by_type_percent")
        
        risk_priority_df = self.filtered_df[['vuln_id', 'vuln_type', 'risk_priority', 
                                            'base_score', 'exploitability_score', 'has_exploit']]
        risk_priority_df.to_csv(f"{self.output_dir}/{priorities_filename}.csv", index=False)
        risk_by_type.to_csv(f"{self.output_dir}/{dist_filename}.csv")
        risk_by_type_pct.to_csv(f"{self.output_dir}/{pct_filename}.csv")
        
        return risk_priority_df, risk_by_type

    def analyze_attack_chains(self):
        """Analyze potential attack chains across different systems."""
        print("Analyzing attack chains...")
        
        # Define attack stages
        attack_stages = {
            'Initial Access': ['Information Disclosure', 'Authentication Issues', 
                              'Open Redirect', 'Input Validation'],
            'Execution': ['Command Injection', 'Code Injection', 'Cross-site Scripting (XSS)', 
                         'SQL Injection', 'Deserialization of Untrusted Data'],
            'Privilege Escalation': ['Access Control Issues', 'Privilege Escalation', 
                                    'Path Traversal', 'SSRF'],
            'Data Exfiltration': ['Information Disclosure', 'Cryptographic Issues', 
                                 'Cleartext Transmission']
        }
        
        # Map vulnerabilities to attack stages
        for stage, vuln_types in attack_stages.items():
            stage_col = f'is_{stage.lower().replace(" ", "_")}'
            self.filtered_df.loc[:, stage_col] = self.filtered_df['vuln_type'].isin(vuln_types)
        
        # Ensure we have product and vendor columns
        if 'products' not in self.filtered_df.columns or 'vendors' not in self.filtered_df.columns:
            print("Warning: product or vendor columns missing, skipping attack chain analysis")
            return None
        
        # Group by product to see which have vulnerabilities in multiple stages
        product_stages = self.filtered_df.groupby(['vendors', 'products'])[
            [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        ].sum()
        
        # Products with complete attack chains have vulnerabilities in all stages
        stage_cols = [f'is_{stage.lower().replace(" ", "_")}' for stage in attack_stages]
        product_stages['attack_chain_completeness'] = (
            product_stages[stage_cols].apply(lambda x: sum(x > 0) / len(attack_stages), axis=1)
        )
        
        # Add vulnerability counts
        product_vuln_count = self.filtered_df.groupby(['vendors', 'products']).size()
        product_stages['total_vulnerabilities'] = product_vuln_count
        
        # Sort by attack chain completeness and then by total vulnerabilities 
        product_stages = product_stages.sort_values(
            ['attack_chain_completeness', 'total_vulnerabilities'], 
            ascending=[False, False]
        )
        
        # Create a visualization of the top N products with most complete attack chains
        top_products = product_stages.head(20)
        
        # Plot attack chain completeness for top products
        plt.figure(figsize=(15, 10))
        
        # Create a more readable index for plotting
        plot_index = [f"{v} {p}" for v, p in top_products.index]
        
        # Plot the completeness as a horizontal bar chart
        ax = plt.barh(plot_index, top_products['attack_chain_completeness'], color='steelblue')
        
        # Add total vulnerability count as text
        for i, (idx, row) in enumerate(top_products.iterrows()):
            plt.text(
                row['attack_chain_completeness'] + 0.01, 
                i, 
                f"({int(row['total_vulnerabilities'])} vulns)", 
                va='center'
            )
        
        plt.title('Top Products by Attack Chain Completeness')
        plt.xlabel('Attack Chain Completeness (proportion of attack stages covered)')
        plt.ylabel('Product')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        plt.xlim(0, 1.1)  # Make room for annotations
        plt.tight_layout()
        
        # Use filter information in filename
        chain_filename = self.get_output_filename("attack_chain_completeness")
        plt.savefig(f"{self.output_dir}/{chain_filename}.png")
        
        # Save detailed product stage data with filter information
        stages_filename = self.get_output_filename("product_attack_stages")
        product_stages.to_csv(f"{self.output_dir}/{stages_filename}.csv")
        
        # Also create a heatmap for the top products showing which stages they're vulnerable to
        plt.figure(figsize=(16, 12))
        
        # Prepare data for heatmap (boolean to show which stages each product has vulns for)
        heatmap_data = top_products[stage_cols].copy()
        # Use apply column by column instead of deprecated applymap
        for col in heatmap_data.columns:
            heatmap_data[col] = heatmap_data[col].apply(lambda x: 1 if x > 0 else 0)
        heatmap_data.columns = [col.replace('is_', '').replace('_', ' ').title() for col in heatmap_data.columns]
        
        sns.heatmap(heatmap_data, cmap=['white', 'darkred'], cbar=False, 
                   linewidths=1, linecolor='lightgray', 
                   yticklabels=plot_index)
        plt.title('Attack Stages Coverage by Top Products')
        plt.tight_layout()
        
        # Use filter information in filename  
        heatmap_filename = self.get_output_filename("attack_stages_heatmap")
        plt.savefig(f"{self.output_dir}/{heatmap_filename}.png")
        
        # Compute average risk score (e.g., base_score) for each product and stage
        risk_matrix = pd.DataFrame(index=top_products.index, columns=stage_cols)
        for (vendor, product) in top_products.index:
            prod_mask = (self.filtered_df['vendors'] == vendor) & (self.filtered_df['products'] == product)
            for stage, col in zip(attack_stages, stage_cols):
                stage_mask = self.filtered_df['vuln_type'].isin(attack_stages[stage])
                relevant = self.filtered_df[prod_mask & stage_mask]
                if not relevant.empty and 'base_score' in relevant.columns:
                    risk_matrix.loc[(vendor, product), col] = relevant['base_score'].mean()
                else:
                    risk_matrix.loc[(vendor, product), col] = np.nan

        # Normalize risk scores for color mapping (optional, for better color scaling)
        risk_matrix = risk_matrix.astype(float)

        # Choose a color palette: green (low) → yellow → red (high)
        cmap = sns.color_palette("RdYlGn_r", as_cmap=True)

        # Prepare labels for the y-axis
        plot_index = [f"{v} {p}" for v, p in risk_matrix.index]
        risk_matrix = risk_matrix.rename(index=dict(zip(risk_matrix.index, plot_index)))
        risk_matrix.columns = [col.replace('is_', '').replace('_', ' ').title() for col in risk_matrix.columns]

        plt.figure(figsize=(16, 12))
        sns.heatmap(
            risk_matrix,
            cmap=cmap,
            linewidths=1,
            linecolor='lightgray',
            yticklabels=plot_index,
            annot=True, fmt=".1f",  # Show average risk score in each cell
            cbar_kws={'label': 'Average Risk Score'}
        )
        plt.title('Attack Stages Coverage by Top Products (Colored by Average Risk Score)')
        plt.tight_layout()

        # Use filter information in filename  
        heatmap_filename = self.get_output_filename("attack_stages_risk_heatmap")
        plt.savefig(f"{self.output_dir}/{heatmap_filename}.png")
        
        return product_stages

    def analyze_mitigation_coverage(self):
        """Analyze which vulnerability types have the most comprehensive mitigations."""
        print("Analyzing mitigation coverage...")
        
        # Check if mitigation_info column exists
        if 'mitigation_info' not in self.filtered_df.columns:
            print("Warning: 'mitigation_info' column missing, skipping mitigation coverage analysis")
            return None
        
        # Calculate mitigation coverage (how complete/detailed the mitigation info is)
        # This assumes mitigation_info has been parsed into a usable format
        self.filtered_df.loc[:, 'has_mitigation'] = self.filtered_df['mitigation_info'].apply(
            lambda x: 0 if pd.isna(x) or x == {} or x == '' else 1
        )
        
        # Group by vulnerability type and calculate coverage percentage
        mitigation_coverage = self.filtered_df.groupby('vuln_type').agg(
            coverage_pct=('has_mitigation', lambda x: sum(x) / len(x) * 100),
            total_vulns=('vuln_type', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        plt.figure(figsize=(14, 8))
        ax = mitigation_coverage['coverage_pct'].plot(kind='barh', color='steelblue')
        plt.title('Mitigation Coverage by Vulnerability Type')
        plt.xlabel('Coverage Percentage')
        plt.ylabel('Vulnerability Type')
        plt.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(mitigation_coverage.iterrows()):
            plt.text(1, i, f"n={row['total_vulns']}", va='center')
        
        plt.tight_layout()
        
        # Use filter information in filename
        coverage_filename = self.get_output_filename("mitigation_coverage")
        plt.savefig(f"{self.output_dir}/{coverage_filename}.png")
        
        # Save to CSV with filter information
        csv_filename = self.get_output_filename("mitigation_coverage")
        mitigation_coverage.to_csv(f"{self.output_dir}/{csv_filename}.csv")
        
        return mitigation_coverage

    def analyze_zero_day_windows(self):
        """Analyze the window between vulnerability publication and patch availability."""
        print("Analyzing zero-day windows...")
        
        # Check if days_to_patch column exists
        if 'days_to_patch' not in self.filtered_df.columns:
            print("Warning: 'days_to_patch' column missing, skipping zero-day window analysis")
            return None
        
        # Create a true/false "has patch" column based on days_to_patch
        self.filtered_df.loc[:, 'has_patch'] = ~self.filtered_df['days_to_patch'].isna() & (self.filtered_df['days_to_patch'] >= 0)
        
        # For vulnerabilities with patches, analyze the waiting period
        patched_vulns = self.filtered_df[self.filtered_df['has_patch']].copy()
        
        if len(patched_vulns) == 0:
            print("No patched vulnerabilities found, skipping zero-day window analysis")
            return None
        
        # Group by vulnerability type and calculate statistics
        patch_stats = patched_vulns.groupby('vuln_type')['days_to_patch'].agg(
            ['mean', 'median', 'min', 'max', 'count']
        ).sort_values('mean')
        
        # Calculate the percentage of vulnerabilities that have patches
        patch_coverage = self.filtered_df.groupby('vuln_type').agg(
            coverage_pct=('has_patch', lambda x: sum(x) / len(x) * 100),
            total_vulns=('has_patch', 'count')
        ).sort_values('coverage_pct')
        
        # Plot the results
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))
        
        # Plot average days to patch
        patch_stats['mean'].plot(kind='barh', ax=ax1, color='steelblue')
        ax1.set_title('Average Days to Patch by Vulnerability Type')
        ax1.set_xlabel('Days')
        ax1.set_ylabel('Vulnerability Type')
        ax1.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_stats.iterrows()):
            ax1.text(1, i, f"n={int(row['count'])}", va='center')
        
        # Plot patch coverage
        patch_coverage['coverage_pct'].plot(kind='barh', ax=ax2, color='darkgreen')
        ax2.set_title('Patch Coverage by Vulnerability Type (%)')
        ax2.set_xlabel('Coverage Percentage')
        ax2.set_ylabel('Vulnerability Type')
        ax2.grid(True, linestyle='--', axis='x', alpha=0.7)
        
        # Add count annotations
        for i, (idx, row) in enumerate(patch_coverage.iterrows()):
            ax2.text(1, i, f"n={int(row['total_vulns'])}", va='center')
        
        plt.tight_layout()
        
        # Use filter information in filename
        patch_filename = self.get_output_filename("patch_analysis")
        plt.savefig(f"{self.output_dir}/{patch_filename}.png")
        
        # Save to CSV with filter information
        timing_filename = self.get_output_filename("vulnerability_patch_timing")
        coverage_filename = self.get_output_filename("vulnerability_patch_coverage")
        
        patch_stats.to_csv(f"{self.output_dir}/{timing_filename}.csv")
        patch_coverage.to_csv(f"{self.output_dir}/{coverage_filename}.csv")
        
        return patch_stats, patch_coverage

    def analyze_temporal_trends(self, months=None):
        """
        Analyze how vulnerability types have evolved over time.
        
        Args:
            months (int, optional): Number of months to look back. If None, all available data is used.
        """
        print("Analyzing temporal vulnerability trends...")
        
        # Create a working copy of the filtered dataframe
        df = self.filtered_df.copy()
        
        # Filter by time window if specified
        if months is not None and months > 0:
            # Find the cutoff date
            latest_date = df['published'].max()
            if pd.isna(latest_date):
                print("Warning: No valid dates found in dataset, using current date")
                latest_date = pd.Timestamp.now(tz='UTC')  # Add timezone info
            elif not isinstance(latest_date, pd.Timestamp):
                # Convert to timestamp if it's not already
                latest_date = pd.Timestamp(latest_date)
                
            # Check if latest_date is timezone-aware, if not make it UTC
            if latest_date.tzinfo is None:
                latest_date = latest_date.tz_localize('UTC')
                
            cutoff_date = latest_date - pd.DateOffset(months=months)
            
            # Make sure cutoff_date has the same timezone as published_date
            # Check a sample of published_date to see if it's timezone-aware
            sample_date = df['published'].dropna().iloc[0] if len(df['published'].dropna()) > 0 else None
            
            if sample_date is not None:
                if sample_date.tzinfo is None and cutoff_date.tzinfo is not None:
                    # If published_date is naive but cutoff is aware, make cutoff naive
                    cutoff_date = cutoff_date.tz_localize(None)
                elif sample_date.tzinfo is not None and cutoff_date.tzinfo is None:
                    # If published_date is aware but cutoff is naive, make cutoff aware
                    cutoff_date = cutoff_date.tz_localize(sample_date.tzinfo)
            
            # Filter data to only include records after the cutoff date
            df = df[df['published'] >= cutoff_date].copy()
            print(f"Filtered to vulnerabilities from the last {months} months ({len(df)} records)")
            
            if len(df) == 0:
                print("No data available for the specified time window")
                return None, None
        
        # Extract year and month for grouping
        df.loc[:, 'year_month'] = df['published'].dt.to_period('M')
        
        # Group by month and vulnerability type
        monthly_trends = df.groupby(['year_month', 'vuln_type']).size().unstack().fillna(0)
        
        # Ensure we have data
        if len(monthly_trends) == 0:
            print("No trend data available to plot")
            return None, None
        
        # Plot the trends
        plt.figure(figsize=(15, 8))
        ax = plt.gca()

        # Convert PeriodIndex to datetime for better plotting
        if isinstance(monthly_trends.index, pd.PeriodIndex):
            monthly_trends.index = monthly_trends.index.to_timestamp()

        # Use a color palette for better distinction
        palette = sns.color_palette("tab20", n_colors=len(monthly_trends.columns))
        color_list = palette.as_hex() if hasattr(palette, "as_hex") else palette

        # Only plot the top N types for clarity
        top_types = monthly_trends.sum().sort_values(ascending=False).head(8).index
        monthly_trends_to_plot = monthly_trends[top_types]

        monthly_trends_to_plot.plot(
            ax=ax, marker='o', linewidth=2, color=color_list[:len(top_types)]
        )

        plt.title('Evolution of Vulnerability Types Over Time')
        plt.xlabel('Month-Year')
        plt.ylabel('Count')
        plt.grid(True, linestyle='--', alpha=0.7)

        # Format x-axis as dates, rotate for readability
        import matplotlib.dates as mdates
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        plt.xticks(rotation=45, ha='right')

        # Legend for only the top types
        ax.legend(title="Top Vulnerability Types", bbox_to_anchor=(1.05, 1), loc='upper left')

        plt.tight_layout()
        
        # Use our new helper function to generate the filename
        filename = self.get_output_filename("temporal_vulnerability_trends", months)
        plt.savefig(f"{self.output_dir}/{filename}.png")
        
        # Calculate month-over-month growth rates for each vulnerability type
        growth_rates = pd.DataFrame()
        for vuln_type in monthly_trends.columns:
            monthly_data = monthly_trends[vuln_type]
            growth = monthly_data.pct_change() * 100  # Convert to percentage
            growth_rates[vuln_type] = growth
        
        # Use our new helper function to generate the filenames
        trends_filename = self.get_output_filename("vulnerability_trends_by_month", months)
        growth_filename = self.get_output_filename("vulnerability_growth_rates_monthly", months)
        
        monthly_trends.to_csv(f"{self.output_dir}/{trends_filename}.csv")
        growth_rates.to_csv(f"{self.output_dir}/{growth_filename}.csv")
        
        return monthly_trends, growth_rates

    def run_all_analyses(self, months=None, vendor_filter=None, product_filter=None):
        """
        Run all analyses and return results.
        
        Args:
            months (int, optional): Number of months to look back for temporal analysis.
            vendor_filter: String or list of strings to filter vendors
            product_filter: String or list of strings to filter products
        """
        # Apply filters if specified
        if vendor_filter or product_filter:
            self.filter_by_vendor_product(vendor_filter, product_filter)
        
        results = {}
        
        # Run basic analysis first
        print("\n1. Running basic vulnerability landscape analysis...")
        results['basic'] = self.analyze_vulnerability_landscape()
        
        # Run the six advanced analyses
        print("\n2. Analyzing temporal vulnerability trends...")
        results['temporal'] = self.analyze_temporal_trends(months=months)
        
        print("\n3. Analyzing vulnerability complexity...")
        results['complexity'] = self.analyze_complexity()
        
        print("\n4. Creating risk prioritization matrix...")
        results['risk_matrix'] = self.create_risk_matrix()
        
        print("\n5. Analyzing attack chains...")
        results['attack_chains'] = self.analyze_attack_chains()
        
        print("\n6. Analyzing mitigation coverage...")
        results['mitigation'] = self.analyze_mitigation_coverage()
        
        print("\n7. Analyzing zero-day windows...")
        results['zero_day'] = self.analyze_zero_day_windows()
        
        print(f"\nAll analyses complete. Reports saved to {self.output_dir}/")
        return results

    def set_exclude_types(self, exclude_types=None):
        """
        Update which vulnerability types to exclude from analysis.
        
        Args:
            exclude_types: List of vulnerability types to exclude or None to reset to default
        """
        if exclude_types is None:
            self.exclude_types = ["Unknown", "Other"]
        else:
            self.exclude_types = exclude_types
        
        # Update filtered dataframe - Create a proper copy to avoid SettingWithCopyWarning
        self.filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)].copy()
        print(f"Updated exclusion list. Now excluding: {self.exclude_types}")
        print(f"Filtered dataset contains {len(self.filtered_df)} records")

    def _smart_text_match(self, series, patterns):
        """
        Perform smart text matching on a pandas Series.
        
        Args:
            series: Pandas Series containing text to match against
            patterns: String or list of strings to match (case-insensitive partial match)
        
        Returns:
            Boolean Series indicating matches
        """
        # Convert patterns to list if it's a string
        if isinstance(patterns, str):
            patterns = [patterns]
        
        # Initialize result array with False values
        matches = pd.Series(False, index=series.index)
        
        # Convert series to lowercase strings
        series_lower = series.fillna('').astype(str).str.lower()
        
        # Check each pattern
        for pattern in patterns:
            pattern_lower = pattern.lower()
            # Match if pattern is contained in the text
            pattern_matches = series_lower.str.contains(pattern_lower, na=False)
            # Combine with OR logic (match any pattern)
            matches = matches | pattern_matches
        
        return matches

    def get_output_filename(self, base_name, months=None):
        """
        Generate an output filename that includes filter information.
        
        Args:
            base_name: Base filename without extension
            months: Optional month filter to include in filename
            
        Returns:
            Enhanced filename with filter information
        """
        filename = base_name
        
        # Add months info if applicable
        if months:
            filename += f"_{months}mo"
        
        # Add filter description if applicable
        if hasattr(self, 'filter_description') and self.filter_description:
            # Clean up the filter description for use in a filename
            clean_desc = self.filter_description.replace('=', '_').replace(',', '-')
            filename += f"_{clean_desc}"
        
        return filename


    def analyze_vulnerability_landscape(self):
        """Analyze the vulnerability landscape including prevalence and severity."""
        print("Analyzing vulnerability landscape...")
        
        # Define severity weights mapping with better null handling
        severity_weights = {
            "LOW": 1, "Low": 1, "low": 1,
            "MEDIUM": 3, "Medium": 3, "medium": 3,
            "HIGH": 6, "High": 6, "high": 6,
            "CRITICAL": 10, "Critical": 10, "critical": 10
        }
        
        # Filter the dataframe if exclusions are specified
        if self.exclude_types:
            filtered_df = self.df[~self.df["vuln_type"].isin(self.exclude_types)]
            print(f"Excluded {len(self.df) - len(filtered_df)} records with types: {self.exclude_types}")
        else:
            filtered_df = self.df
        
        # Vulnerability type distribution (prevalence)
        plt.figure(figsize=(14, 8))
        type_counts = filtered_df["vuln_type"].value_counts()
        
        # Plot the bar chart
        sns.barplot(y=type_counts.index, x=type_counts.values, color="steelblue")
        plt.title("Distribution of Vulnerability Types (Prevalence)")
        plt.xlabel("Count")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        
        # Use filter information in filename
        filename = self.get_output_filename("vuln_type_distribution")
        plt.savefig(f"{self.output_dir}/{filename}.png")
        
        # Create a dataframe with type, count and weighted score
        risk_scores = []
        for vuln_type in filtered_df["vuln_type"].unique():
            type_df = filtered_df[filtered_df["vuln_type"] == vuln_type]
            count = len(type_df)
            
            # Calculate severity score with better robustness
            if "severity" in type_df.columns:
                # Convert to string and map to weights with robust error handling
                valid_severities = type_df["severity"].dropna().astype(str)
                if len(valid_severities) > 0:
                    # Map severities to weights, defaulting to 0 for unknown values
                    severity_values = valid_severities.map(
                        lambda x: severity_weights.get(x, 0)
                    ).fillna(0)
                    
                    # Calculate average severity weight (if there are valid values)
                    if len(severity_values) > 0 and severity_values.sum() > 0:
                        weighted_score = severity_values.sum() / len(severity_values)
                    else:
                        weighted_score = 0
                else:
                    weighted_score = 0
            else:
                # If no severity column, use base_score as a proxy if available
                if "base_score" in type_df.columns:
                    valid_scores = type_df["base_score"].dropna()
                    if len(valid_scores) > 0:
                        # Normalize base score (0-10) to our weight scale (1-10)
                        weighted_score = valid_scores.mean() * (10/10)
                    else:
                        weighted_score = 0
                else:
                    weighted_score = 0
            
            # Risk score is count * severity weight
            risk_score = count * weighted_score
            
            risk_scores.append({
                "vuln_type": vuln_type,
                "count": count,
                "avg_severity_weight": weighted_score,
                "risk_score": risk_score
            })
        
        risk_df = pd.DataFrame(risk_scores).sort_values("risk_score", ascending=False)
        
        # Plot combined risk score
        plt.figure(figsize=(14, 8))
        sns.barplot(y="vuln_type", x="risk_score", data=risk_df, color="purple")
        plt.title("Combined Risk Score by Vulnerability Type (Prevalence × Severity)")
        plt.xlabel("Risk Score")
        plt.ylabel("Vulnerability Type")
        plt.tight_layout()
        
        # Use filter information in filename  
        risk_filename = self.get_output_filename("vuln_type_risk_score")
        plt.savefig(f"{self.output_dir}/{risk_filename}.png")
        
        # Save results with filter information in filename
        results_filename = self.get_output_filename("vulnerability_risk_analysis")
        risk_df.to_csv(f"{self.output_dir}/{results_filename}.csv", index=False)
        print(f"Risk analysis saved to {self.output_dir}/{results_filename}.csv")
        
        print("\nTop 5 vulnerability types by combined risk score:")
        print(risk_df[["vuln_type", "count", "avg_severity_weight", "risk_score"]].head(5))
        
        return risk_df
def parse_cvss_vector_fields(df, vector_col='cvss_vector'):
    """
    Parse CVSS vector strings and extract key fields (AC, UI, AV, PR, etc.) into new columns.
    Args:
        df: pandas DataFrame containing a column with CVSS vector strings.
        vector_col: name of the column containing the CVSS vector.
    Returns:
        DataFrame with new columns: 'ac', 'ui', 'av', 'pr', 's', 'c', 'i', 'a'
    """
    # CVSS v3 vector example: "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H"
    # CVSS v2 vector example: "AV:N/AC:L/Au:N/C:P/I:P/A:P"
    def extract_cvss_fields(vector):
        from typing import Optional, Dict
        fields: Dict[str, Optional[str]] = {'ac': None, 'ui': None, 'av': None, 'pr': None, 's': None, 'c': None, 'i': None, 'a': None}
        if not isinstance(vector, str):
            return fields
        # Remove CVSS version prefix if present
        vector = re.sub(r'^CVSS:[\d.]+/', '', vector)
        for part in vector.split('/'):
            if ':' in part:
                k, v = part.split(':', 1)
                k = k.lower()
                v = v.upper()
                if k == 'ac':
                    fields['ac'] = v
                elif k == 'ui':
                    fields['ui'] = v
                elif k == 'av':
                    fields['av'] = v
                elif k == 'pr':
                    fields['pr'] = v
                elif k == 's':
                    fields['s'] = v
                elif k == 'c':
                    fields['c'] = v
                elif k == 'i':
                    fields['i'] = v
                elif k == 'a':
                    fields['a'] = v
        return fields

    cvss_fields = df[vector_col].apply(extract_cvss_fields)
    cvss_df = pd.DataFrame(list(cvss_fields))
    for col in cvss_df.columns:
        df[col] = cvss_df[col]
    return df
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='TWX Advanced Vulnerability Analysis')
    parser.add_argument('--data', type=str, default="analysis/classification_data.json",
                  help='Path to the classified vulnerability data (JSON or CSV)')
    parser.add_argument('--analyses', type=str, default="all",
                      help='Comma-separated list of analyses to run, or "all"')
    parser.add_argument('--exclude', type=str, default=None,
                      help='Comma-separated list of vulnerability types to exclude')
    parser.add_argument('--output', type=str, default="analysis/reports",
                      help='Directory where reports will be saved')
    parser.add_argument('--months', type=int, default=None,
                      help='Number of months to look back for temporal analysis')
    parser.add_argument('--vendor', type=str, default=None,
                      help='Vendor filter (comma-separated for multiple patterns)')
    parser.add_argument('--product', type=str, default=None,
                      help='Product filter (comma-separated for multiple patterns)')
    
    args = parser.parse_args()
    
    # Process vendor and product filters
    vendor_filter = None
    if args.vendor:
        vendor_filter = [v.strip() for v in args.vendor.split(',')]
    
    product_filter = None
    if args.product:
        product_filter = [p.strip() for p in args.product.split(',')]
    
    try:
        # Initialize the analyzer with data path only
        analyzer = VulnerabilityAnalyzer(data_path=args.data)
        
        # Apply filters if specified (using the dedicated method)
        if vendor_filter or product_filter:
            analyzer.filter_by_vendor_product(vendor_filter, product_filter)
        
        # Set output directory if specified
        if args.output:
            analyzer.output_dir = args.output
            os.makedirs(analyzer.output_dir, exist_ok=True)
        
        # Set exclusion types if specified
        if args.exclude:
            exclude_list = [t.strip() for t in args.exclude.split(',')]
            analyzer.set_exclude_types(exclude_list)
        
        print(f"Ready to analyze {len(analyzer.filtered_df)} vulnerability records")
        
        # Run analyses based on arguments
        if args.analyses.lower() == "all":
            # Run all analyses, passing the months parameter to temporal trends
            analyzer.analyze_vulnerability_landscape()
            analyzer.analyze_temporal_trends(months=args.months)
            analyzer.analyze_complexity()
            analyzer.create_risk_matrix()
            analyzer.analyze_attack_chains()
            analyzer.analyze_mitigation_coverage()
            analyzer.analyze_zero_day_windows()
            analyzer.analyze_risk_vs_complexity()
            analyzer.analyze_attack_graphs()
            #analyzer.analyze_contextual_priorities()

        else:
            # Run selected analyses
            analyses = args.analyses.split(',')
            for analysis in analyses:
                analysis = analysis.strip().lower()
                if analysis == "basic":
                    analyzer.analyze_vulnerability_landscape()
                elif analysis == "temporal":
                    analyzer.analyze_temporal_trends(months=args.months)
                elif analysis == "complexity":
                    analyzer.analyze_complexity()
                elif analysis == "risk":
                    analyzer.create_risk_matrix()
                elif analysis == "attack_chains":
                    analyzer.analyze_attack_chains()
                elif analysis == "mitigation":
                    analyzer.analyze_mitigation_coverage()
                elif analysis == "zero_day":
                    analyzer.analyze_zero_day_windows()
                elif analysis == "attack_graph" or analysis == "attack_graphs":  # Add this
                    analyzer.analyze_attack_graphs()
                else:
                    print(f"Warning: Unknown analysis type '{analysis}'")
    
    except Exception as e:
        print(f"Error running analysis: {e}")
        import traceback
        traceback.print_exc()